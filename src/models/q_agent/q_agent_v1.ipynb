{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4db09d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import random\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316ccab3",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9687088",
   "metadata": {},
   "source": [
    "**Baseline config** initial state:  (used in production)   \n",
    "3 PIDs (engine load, rpm, speed) set to monitoring, with 300 s Min Period (distance, angle, speed delta are not used), min saved records = 1, send period = 120.\n",
    "\n",
    "**Worst config**  initial state (from my own experience)  \n",
    "3 PIDs (engine load, rpm, speed) set to on_change, with 1 s Min Period (distance, angle, speed delta are not used), min saved records = 1, send period = 1.\n",
    "\n",
    "**Test conditions**\n",
    "1. \"Vehicle trip\" (12 minutes vehicle trip with values increasing monotonically)\n",
    "2.  Sleep, battery, GPS precision and network mode (home, roaming, unkown) are assumed optimal. \n",
    "\n",
    "**State representation**  \n",
    "1. $p_{i_0}$----PID (integer identifier for PID, e.g. 12 = RPM)\n",
    "2. $p_{i_1}$----enable PID $i$   (0 or 1)\n",
    "3. $p_{i_2}$----change data acquisition strategy for PID $i$  *(monitoring, on_change, hysteresis, delta_change)*  \n",
    "4. $d_0$------increase/decrease min_time  (time the device lets pass before querying PID values)     *(seconds, 0 disables it)*  \n",
    "5. $d_1$------increase/decrease min_saved (number of records the device will accumulate before sending) *(int, 0 disables it)*  \n",
    "6. $d_2$------increase/decrease send_time  (time the device lets pass before attempting to send a new record)  *(seconds, 0 disables it)*  \n",
    "7. $d_3$------reset baseline  (Device)   *reset to a known stable configuration*  (0,1)\n",
    " \n",
    "\n",
    "For example, considering 3 different PIDs a state $S$ is defined like: \n",
    "\n",
    "$S = (p_{00}, p_{01}, p_{02}, p_{10}, p_{11}, p_{12}, p_{20}, p_{21}, p_{22}, d_0, d_1, d_2, d_3)$\n",
    "\n",
    "So for 3 PIDs, the $S$ vector is of size $13$, and in general:  $3n+ 4$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0a4641",
   "metadata": {},
   "source": [
    "### PID configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf3a675",
   "metadata": {},
   "outputs": [],
   "source": [
    "STRATEGY_MAP = {\n",
    "    \"monitoring\": 0,\n",
    "    \"on_change\": 1,\n",
    "    \"hysteresis\": 2,\n",
    "    \"on_delta_change\": 3 # on_enter, on_exit, on_both are omitted to simplify the model.\n",
    "}\n",
    "\n",
    "REVERSE_STRATEGY_MAP = {v: k for k, v in STRATEGY_MAP.items()}\n",
    "STRATEGY_LIST = list(STRATEGY_MAP.keys())\n",
    "\n",
    "ACTIVE_PIDS = [12, 13, 39]  # add all necessary PIDs\n",
    "n_pids = len(ACTIVE_PIDS)\n",
    "\n",
    "PID_PRECISION = {\n",
    "    12: 100,\n",
    "    13: 5,\n",
    "    39: 5,\n",
    "    28: 2,\n",
    "    38: 3,\n",
    "    31: 3\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f36e25",
   "metadata": {},
   "source": [
    "### State encoder / decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf41bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_config(config: dict) -> list:\n",
    "    \"\"\"\n",
    "    Converts a human-readable config dict into a state vector.\n",
    "    \"\"\"\n",
    "    vector = []\n",
    "\n",
    "    # Encode OBD parameters\n",
    "    for param in config.get(\"obd_parameters\", []):\n",
    "        pid = param.get(\"pid\", 0)\n",
    "        enabled = 1 if param.get(\"enabled\", False) else 0\n",
    "        strategy_idx = STRATEGY_MAP.get(param.get(\"strategy\", \"monitoring\"), 0)\n",
    "        vector.extend([pid, enabled, strategy_idx])\n",
    "\n",
    "    # Global settings\n",
    "    global_ = config.get(\"global_settings\", {})\n",
    "    vector.extend([\n",
    "        global_.get(\"min_time\", 10),\n",
    "        global_.get(\"send_period\", 10),\n",
    "        global_.get(\"min_saved_records\", 1)\n",
    "    ])\n",
    "\n",
    "    return vector\n",
    "\n",
    "\n",
    "def decode_config(vector: list) -> dict:\n",
    "    \"\"\"\n",
    "    Converts a state vector into a human-readable config dict.\n",
    "    \"\"\"\n",
    "    config = {\"obd_parameters\": [], \"global_settings\": {}}\n",
    "    n_pids = len(ACTIVE_PIDS)\n",
    "\n",
    "    # Decode OBD parameter block\n",
    "    for i in range(n_pids):\n",
    "        base = i * 3\n",
    "        pid, enabled, strategy_idx = vector[base:base + 3]\n",
    "        config[\"obd_parameters\"].append({\n",
    "            \"pid\": pid,\n",
    "            \"enabled\": bool(enabled),\n",
    "            \"strategy\": REVERSE_STRATEGY_MAP.get(strategy_idx, \"monitoring\")\n",
    "        })\n",
    "\n",
    "    # Decode global settings\n",
    "    g_base = n_pids * 3\n",
    "    config[\"global_settings\"] = {\n",
    "        \"min_time\": vector[g_base],\n",
    "        \"send_period\": vector[g_base + 1],\n",
    "        \"min_saved_records\": vector[g_base + 2]\n",
    "    }\n",
    "\n",
    "    return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0300928",
   "metadata": {},
   "source": [
    "### Actions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4d2b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTIONS = []\n",
    "\n",
    "# Toggle PID enable flag\n",
    "for pid in [12, 13, 39]:\n",
    "    ACTIONS.append({\"type\": \"toggle\", \"pid\": pid})\n",
    "\n",
    "# Cycle PID strategy\n",
    "for pid in [12, 13, 39]:\n",
    "    ACTIONS.append({\"type\": \"cycle_strategy\", \"pid\": pid})\n",
    "\n",
    "# Adjust global parameters\n",
    "ACTIONS.extend([\n",
    "    {\"type\": \"adjust\", \"param\": \"min_time\", \"delta\": -60},\n",
    "    {\"type\": \"adjust\", \"param\": \"min_time\", \"delta\": 60},\n",
    "    {\"type\": \"adjust\", \"param\": \"send_period\", \"delta\": -30},\n",
    "    {\"type\": \"adjust\", \"param\": \"send_period\", \"delta\": 30}\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804282f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pids = len(ACTIVE_PIDS)\n",
    "g_base = n_pids * 3  # index where global settings start\n",
    "\n",
    "# Per-PID toggle: action_id in 0 to n_pids - 1\n",
    "# Per-PID strategy: action_id in n_pids to 2 * n_pids - 1\n",
    "# Global actions: fixed offsets starting from 2 * n_pids\n",
    "\n",
    "def apply_action(state: list[int], action_id: int, trace: bool = True, baseline: list[int] = None) -> tuple[list[int], dict]:\n",
    "    \"\"\"\n",
    "    Applies a single action to the config vector and returns (new_state, trace_dict).\n",
    "    The state layout is: [pid, enabled, strategy_idx] * N + [min_time, send_period, min_saved_records]\n",
    "    \"\"\"\n",
    "    new_vector = deepcopy(state)\n",
    "    trace_info = {}\n",
    "\n",
    "    n_pids = len(ACTIVE_PIDS)\n",
    "    g_base = n_pids * 3\n",
    "\n",
    "    def log(msg):\n",
    "        if trace:\n",
    "            print(f\"[apply_action] {msg}\")\n",
    "        trace_info[\"change\"] = msg\n",
    "\n",
    "    # PID toggle: action 0 to N-1\n",
    "    if action_id < n_pids:\n",
    "        idx = action_id\n",
    "        base = idx * 3\n",
    "        new_vector[base + 1] = 1 - new_vector[base + 1]\n",
    "        log(f\"Toggled PID {new_vector[base]} enable → {new_vector[base + 1]}\")\n",
    "        return new_vector, trace_info\n",
    "\n",
    "    # PID strategy cycle: action N to 2N-1\n",
    "    elif action_id < 2 * n_pids:\n",
    "        idx = action_id - n_pids\n",
    "        base = idx * 3\n",
    "        current = new_vector[base + 2]\n",
    "        new_vector[base + 2] = (current + 1) % len(STRATEGY_LIST)\n",
    "        log(f\"PID {new_vector[base]} strategy: {STRATEGY_LIST[current]} → {STRATEGY_LIST[new_vector[base + 2]]}\")\n",
    "        return new_vector, trace_info\n",
    "\n",
    "    # Global actions start at 2 * n_pids\n",
    "    global_actions = {\n",
    "        0: (\"min_time\", g_base, 5, 300),\n",
    "        1: (\"min_time\", g_base, -5, 1),\n",
    "        2: (\"send_period\", g_base + 1, 5, 300),\n",
    "        3: (\"send_period\", g_base + 1, -5, 1),\n",
    "        4: (\"min_saved_records\", g_base + 2, 1, 10),\n",
    "        5: (\"min_saved_records\", g_base + 2, -1, 1),\n",
    "    }\n",
    "\n",
    "    global_action_id = action_id - 2 * n_pids\n",
    "\n",
    "    if global_action_id in global_actions:\n",
    "        label, index, delta, limit = global_actions[global_action_id]\n",
    "        old_val = new_vector[index]\n",
    "        new_val = old_val + delta\n",
    "        new_val = max(min(new_val, max(limit, old_val)), min(limit, old_val))\n",
    "        new_vector[index] = new_val\n",
    "        log(f\"{label} changed from {old_val} → {new_val}\")\n",
    "        return new_vector, trace_info\n",
    "\n",
    "    # Reset to baseline\n",
    "    if global_action_id == 6 and baseline is not None:\n",
    "        new_vector = deepcopy(baseline)\n",
    "        log(\"Reset to baseline configuration\")\n",
    "        return new_vector, trace_info\n",
    "\n",
    "    log(f\"No change for action_id={action_id}\")\n",
    "    return new_vector, trace_info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c96c8ac",
   "metadata": {},
   "source": [
    "### Reward function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b52f8ac",
   "metadata": {},
   "source": [
    "reward = latency_score + sum(pid_scores)\n",
    "\n",
    "For example:    \n",
    "    latency_ms = 1850\n",
    "```json\n",
    "    pid_data_list = [\n",
    "        {\n",
    "            \"pid\": 12,\n",
    "            \"values\": [1500, 1500, 1500],\n",
    "            \"strategy\": \"on_change\",\n",
    "            \"precision\": 100,\n",
    "            \"valid_range\": (800, 6000)\n",
    "        },\n",
    "        // rest of the PIDs\n",
    "]\n",
    "```\n",
    "\n",
    "**Precision**  \n",
    "This defines how much a value must change to be considered “meaningful.”\n",
    "Even if all values are valid, we may want to ignore tiny fluctuations (e.g., RPM changes by 1 unit) and log only useful variation (e.g., ±100 RPM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037d3c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def latency_score(latency_ms: float) -> float:\n",
    "    \"\"\"\n",
    "    Reward stable and reasonable latency.\n",
    "    - Under 5s: full reward\n",
    "    - Between 5s–20s: moderate reward\n",
    "    - 20s–30s: penalize\n",
    "    - Over 30s: heavy penalty\n",
    "    \"\"\"\n",
    "    if latency_ms <= 5000:\n",
    "        return 1.0\n",
    "    elif latency_ms <= 20000:\n",
    "        return 0.6\n",
    "    elif latency_ms <= 30000:\n",
    "        return -0.3\n",
    "    else:\n",
    "        return -1.0\n",
    "\n",
    "\n",
    "def global_config_penalty(send_period: int, min_time: int, min_saved: int) -> float:\n",
    "    \"\"\"\n",
    "    Penalizes or rewards based on how close parameters are to ideal ranges.\n",
    "    - Ideal:\n",
    "        send_period ≈ 60–120\n",
    "        min_time ≈ 60–300\n",
    "        min_saved ≈ 2–4\n",
    "    \"\"\"\n",
    "    score = 0.0\n",
    "\n",
    "    # --- min_saved_records ---\n",
    "    if min_saved < 2:\n",
    "        score -= 0.4  # too aggressive\n",
    "    elif min_saved > 4:\n",
    "        score -= 0.5  # queues too much\n",
    "    else:\n",
    "        score += 0.1  # ideal\n",
    "\n",
    "    # --- send_period ---\n",
    "    if send_period < 10:\n",
    "        score -= 0.6\n",
    "    elif send_period < 30:\n",
    "        score -= 0.3\n",
    "    elif send_period > 150:\n",
    "        score -= 0.4\n",
    "    elif 60 <= send_period <= 120:\n",
    "        score += 0.2  # optimal\n",
    "    else:\n",
    "        score += 0.05  # acceptable\n",
    "\n",
    "    # --- min_time ---\n",
    "    if min_time < 10:\n",
    "        score -= 0.3\n",
    "    elif min_time > 600:\n",
    "        score -= 0.2\n",
    "    elif 60 <= min_time <= 300:\n",
    "        score += 0.2\n",
    "    else:\n",
    "        score += 0.05\n",
    "\n",
    "    return round(score, 3)\n",
    "    \n",
    "\n",
    "def compute_pid_scores(pid_data_list: list[dict], send_period: int) -> list[dict]:\n",
    "    \"\"\"Computes per-PID scores using updated quality logic.\"\"\"\n",
    "    scores = []\n",
    "    for entry in pid_data_list:\n",
    "        score = data_quality_score(\n",
    "            values=entry[\"values\"],\n",
    "            strategy=entry[\"strategy\"],\n",
    "            precision=entry[\"precision\"],\n",
    "            valid_range=entry[\"valid_range\"],\n",
    "            send_period=send_period\n",
    "        )\n",
    "        scores.append({\n",
    "            \"pid\": entry[\"pid\"],\n",
    "            \"score\": round(score, 3)\n",
    "        })\n",
    "    return scores\n",
    "\n",
    "\n",
    "def data_quality_score(\n",
    "    values: list[float],\n",
    "    strategy: str,\n",
    "    precision: float,\n",
    "    valid_range: tuple[float, float],\n",
    "    send_period: int,\n",
    ") -> float:\n",
    "    \"\"\"Scores PID quality: range check + smooth variation proportional to sampling interval.\"\"\"\n",
    "    if not values:\n",
    "        return -1.0\n",
    "\n",
    "    # 1. All values must be in valid range and non-zero\n",
    "    if not all(valid_range[0] <= v <= valid_range[1] for v in values):\n",
    "        return -1.0\n",
    "    if all(v == 0 for v in values):\n",
    "        return -1.0\n",
    "\n",
    "    # 2. Variation scaled to time gap\n",
    "    variation = max(values) - min(values)\n",
    "    allowed_variation = precision * (send_period / 10)  # allow bigger jumps for longer intervals\n",
    "    significant = variation >= allowed_variation\n",
    "\n",
    "    # 3. Strategy reward logic\n",
    "    match strategy:\n",
    "        case \"on_change\":\n",
    "            return 1.0 if not significant else 0.5\n",
    "        case \"on_delta_change\":\n",
    "            return 1.0 if significant else -0.5\n",
    "        case \"hysteresis\":\n",
    "            return 0.8 if significant else 0.2\n",
    "        case \"monitoring\":\n",
    "            return 0.6 if significant else -0.2\n",
    "\n",
    "    return 0.0  # fallback\n",
    "\n",
    "\n",
    "def compute_average_quality(pid_data_list: list[dict], send_period: int) -> float:\n",
    "    \"\"\"Returns sum of PID scores (no weighting).\"\"\"\n",
    "    scores = [\n",
    "        data_quality_score(\n",
    "            values=entry[\"values\"],\n",
    "            strategy=entry[\"strategy\"],\n",
    "            precision=entry[\"precision\"],\n",
    "            valid_range=entry[\"valid_range\"],\n",
    "            send_period=send_period\n",
    "        )\n",
    "        for entry in pid_data_list\n",
    "    ]\n",
    "    return round(sum(scores), 3) if scores else 0.0\n",
    "\n",
    "\n",
    "def clip_reward(score, min_val=-1.0, max_val=3.0):\n",
    "    \"\"\"Optional clipping to stabilize training and limit outliers.\"\"\"\n",
    "    return max(min(score, max_val), min_val)\n",
    "\n",
    "\n",
    "def compute_reward(latency_ms: int, pid_data_list: list[dict], send_period: int, min_time: int, min_saved: int) -> float:\n",
    "    latency = latency_score(latency_ms)\n",
    "    quality_sum = compute_average_quality(pid_data_list, send_period)\n",
    "    penalty = global_config_penalty(send_period, min_time, min_saved)\n",
    "    return round(latency + quality_sum + penalty, 3)\n",
    "\n",
    "\n",
    "def compute_reward_with_details(latency_ms: int, pid_data_list: list[dict], send_period: int, \n",
    "                                min_time: int, min_saved: int) -> tuple[float, dict]:\n",
    "    for entry in pid_data_list:\n",
    "        entry[\"precision\"] = PID_PRECISION.get(entry[\"pid\"], 1)\n",
    "\n",
    "    latency = latency_score(latency_ms)\n",
    "    pid_scores = compute_pid_scores(pid_data_list, send_period)\n",
    "    total_pid_score = sum(entry[\"score\"] for entry in pid_scores)\n",
    "    config_penalty = global_config_penalty(send_period, min_time, min_saved)\n",
    "\n",
    "    total_reward = round(latency + total_pid_score + config_penalty, 3)\n",
    "\n",
    "    breakdown = {\n",
    "        \"latency_ms\": latency_ms,\n",
    "        \"latency_score\": round(latency, 3),\n",
    "        \"pid_scores\": pid_scores,\n",
    "        \"total_pid_score\": round(total_pid_score, 3),\n",
    "        \"global_config_penalty\": config_penalty,\n",
    "    }\n",
    "\n",
    "    return total_reward, breakdown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf755ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flag_latency_outliers(freq_df: pd.DataFrame, threshold_ms: int = 7_200_000) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds a boolean column 'is_latency_outlier' to the input DataFrame,\n",
    "    where True indicates latency greater than the given threshold (default: 2 hours).\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert timestamps\n",
    "    freq_df[\"ts_recorded\"] = pd.to_datetime(freq_df[\"ts_recorded\"])\n",
    "    freq_df[\"ts_uploaded\"] = pd.to_datetime(freq_df[\"ts_uploaded\"])\n",
    "\n",
    "    # Compute latency in milliseconds\n",
    "    freq_df[\"latency_ms\"] = (freq_df[\"ts_uploaded\"] - freq_df[\"ts_recorded\"]).dt.total_seconds() * 1000\n",
    "\n",
    "    # Flag all rows with latency above the threshold\n",
    "    freq_df[\"is_latency_outlier\"] = freq_df[\"latency_ms\"] > threshold_ms\n",
    "\n",
    "    return freq_df\n",
    "\n",
    "\n",
    "def get_latency(start_ts, end_ts, method='median'):\n",
    "    freq_df = pd.read_csv('../../data_proc/csv_data/qa_device/frequencies.csv', low_memory=False)\n",
    "    freq_df.drop('Unnamed: 0', inplace=True, axis=1)\n",
    "\n",
    "    # Parse timestamps\n",
    "    freq_df[\"ts_recorded\"] = pd.to_datetime(freq_df[\"ts_recorded\"])\n",
    "    freq_df[\"ts_uploaded\"] = pd.to_datetime(freq_df[\"ts_uploaded\"])\n",
    "\n",
    "    # Filter within time range\n",
    "    mask = (freq_df[\"ts_recorded\"] >= pd.to_datetime(start_ts)) & (freq_df[\"ts_recorded\"] <= pd.to_datetime(end_ts))\n",
    "    filtered_df = freq_df[mask].copy()\n",
    "\n",
    "    if filtered_df.empty:\n",
    "        return None  # or raise an exception if preferred\n",
    "\n",
    "    # Compute latency in ms\n",
    "    filtered_df[\"latency_ms\"] = (filtered_df[\"ts_uploaded\"] - filtered_df[\"ts_recorded\"]).dt.total_seconds() * 1000\n",
    "\n",
    "    if method == 'median':\n",
    "        return filtered_df[\"latency_ms\"].median()\n",
    "    elif method == 'mean':\n",
    "        return filtered_df[\"latency_ms\"].mean()\n",
    "    else:\n",
    "        raise ValueError(\"method must be 'median' or 'mean'\")\n",
    "\n",
    "\n",
    "# Define a field-to-PID mapping\n",
    "FIELD_TO_PID = {\n",
    "    \"obd.rpm.value\": 12,\n",
    "    \"obd.speed.value\": 13,\n",
    "    \"obd.fuel_level.value\": 28,\n",
    "    \"obd.coolant_temp.value\": 38,\n",
    "    \"obd.engine_load.value\": 39,\n",
    "    \"obd.intake_temp.value\": 20,\n",
    "    \"obd.maf.value\": 21,\n",
    "    \"obd.throttle_pos.value\": 41,\n",
    "    \"obd.ambient_air_temp.value\": 131,\n",
    "    \"obd.distance_since_codes_clear.value\": 31,\n",
    "    \"obd.time_since_codes_cleared.value\": 47,\n",
    "    # Add more as needed...\n",
    "}\n",
    "\n",
    "\n",
    "def extract_pid_statistics(obd_df: pd.DataFrame, start_ts: str, end_ts: str) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Extracts per-PID reward inputs (PID ID, values, precision, valid range)\n",
    "    from an obd_export dataframe filtered by timestamp range.\n",
    "    Returns a list of dictionaries ready for reward scoring.\n",
    "    \"\"\"\n",
    "    # Filter by time window\n",
    "    obd_df[\"@ts\"] = pd.to_datetime(obd_df[\"@ts\"])\n",
    "    mask = (obd_df[\"@ts\"] >= pd.to_datetime(start_ts)) & (obd_df[\"@ts\"] <= pd.to_datetime(end_ts))\n",
    "    obd_df = obd_df[mask].copy()\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for field, pid in FIELD_TO_PID.items():\n",
    "        if field not in obd_df.columns:\n",
    "            continue\n",
    "\n",
    "        values = obd_df[field].dropna().astype(float).tolist()\n",
    "        if not values:\n",
    "            continue\n",
    "\n",
    "        vmin, vmax = min(values), max(values)\n",
    "        vrange = vmax - vmin\n",
    "\n",
    "        precision = round(vrange * 0.1, 3) if vrange > 0 else 1.0\n",
    "        valid_range = (vmin - vrange * 0.1, vmax + vrange * 0.1)\n",
    "\n",
    "        results.append({\n",
    "            \"pid\": pid,\n",
    "            \"values\": values,\n",
    "            \"precision\": precision,\n",
    "            \"valid_range\": valid_range\n",
    "        })\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7fcb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_pid_statistics(pid_data_list: list[dict]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns a DataFrame summarizing each PID's stats (excluding raw values).\n",
    "    \"\"\"\n",
    "    summary = []\n",
    "\n",
    "    for entry in pid_data_list:\n",
    "        values = entry[\"values\"]\n",
    "        pid_summary = {\n",
    "            \"PID\": entry[\"pid\"],\n",
    "            \"Count\": len(values),\n",
    "            \"Min\": min(values) if values else None,\n",
    "            \"Max\": max(values) if values else None,\n",
    "            \"Precision\": PID_PRECISION.get(entry[\"pid\"], 1),\n",
    "            \"Valid Range\": entry[\"valid_range\"],\n",
    "        }\n",
    "        summary.append(pid_summary)\n",
    "\n",
    "    return pd.DataFrame(summary).sort_values(by=\"PID\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e563591f",
   "metadata": {},
   "source": [
    "### Q-Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3979bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAgent:\n",
    "    def __init__(self, action_space_size, epsilon=0.2, alpha=0.5, gamma=0.9):\n",
    "        self.q_table = defaultdict(lambda: [0.0] * action_space_size)\n",
    "        self.epsilon = epsilon  # exploration rate NOTE tune better the parameters\n",
    "        self.alpha = alpha      # learning rate\n",
    "        self.gamma = gamma      # discount factor\n",
    "        self.action_space_size = action_space_size\n",
    "\n",
    "    def select_action(self, state_vector: list[int]) -> int:\n",
    "        state_key = tuple(state_vector)\n",
    "\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, self.action_space_size - 1)  # explore\n",
    "        else:\n",
    "            q_values = self.q_table[state_key]\n",
    "            return int(q_values.index(max(q_values)))  # exploit\n",
    "\n",
    "    def update(self, state: list[int], action: int, reward: float, next_state: list[int]):\n",
    "        state_key = tuple(state)\n",
    "        next_state_key = tuple(next_state)\n",
    "\n",
    "        old_value = self.q_table[state_key][action]\n",
    "        next_max = max(self.q_table[next_state_key])\n",
    "\n",
    "        # Q-learning update rule\n",
    "        new_value = old_value + self.alpha * (reward + self.gamma * next_max - old_value)\n",
    "        self.q_table[state_key][action] = new_value\n",
    "\n",
    "    def decay_epsilon(self, decay_rate=0.99):\n",
    "        self.epsilon *= decay_rate\n",
    "\n",
    "    def save_q_table(self, path='q_table.json'):\n",
    "        # Convert keys to strings for JSON serialization\n",
    "        json_q = {str(k): v for k, v in self.q_table.items()}\n",
    "        with open(path, 'w') as f:\n",
    "            json.dump(json_q, f, indent=2)\n",
    "\n",
    "    def load_q_table(self, path='q_table.json'):\n",
    "        with open(path, 'r') as f:\n",
    "            json_q = json.load(f)\n",
    "        self.q_table = defaultdict(lambda: [0.0] * self.action_space_size)\n",
    "        for k, v in json_q.items():\n",
    "            self.q_table[tuple(eval(k))] = v\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb272bc6",
   "metadata": {},
   "source": [
    "### Vehicle trip simulation / Q-Agent training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d695c6c",
   "metadata": {},
   "source": [
    "1. Start from a known config (state)\n",
    "2. Select an action with ε-greedy\n",
    "3. Applies it to get next_state\n",
    "4. Run run_vehicle_sim() to compute reward\n",
    "5. Update Q-table\n",
    "6. Repeat for N episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfa7f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_vehicle_sim(config_vector, trip_df, trace=False):\n",
    "    config = decode_config(config_vector)\n",
    "    pid_data_list = []\n",
    "\n",
    "    # Parse timestamps\n",
    "    trip_df[\"ts_recorded\"] = pd.to_datetime(trip_df[\"ts_recorded\"], utc=True, errors=\"coerce\")\n",
    "    trip_df[\"ts_uploaded\"] = pd.to_datetime(trip_df[\"ts_uploaded\"], utc=True, errors=\"coerce\")\n",
    "    trip_df = trip_df.dropna(subset=[\"ts_recorded\", \"ts_uploaded\"])\n",
    "\n",
    "    if trip_df.empty:\n",
    "        if trace:\n",
    "            print(\"[run_vehicle_sim] trip_df is EMPTY after filtering.\")\n",
    "        return -0.5, {\n",
    "            \"latency_ms\": float(\"nan\"),\n",
    "            \"latency_score\": -0.5,\n",
    "            \"pid_scores\": [],\n",
    "            \"total_pid_score\": 0.0,\n",
    "            \"global_config_score\": -1.0,\n",
    "        }\n",
    "\n",
    "    # Extract values from trip log for enabled PIDs\n",
    "    for param in config[\"obd_parameters\"]:\n",
    "        if not param[\"enabled\"]:\n",
    "            continue\n",
    "\n",
    "        pid = param[\"pid\"]\n",
    "        strategy = param[\"strategy\"]\n",
    "\n",
    "        if pid == 12:\n",
    "            values = trip_df[\"obd_rpm\"].tolist()\n",
    "            valid_range = (0, 8000)\n",
    "        elif pid == 13:\n",
    "            values = trip_df[\"obd_speed\"].tolist()\n",
    "            valid_range = (0, 250)\n",
    "        elif pid == 39:\n",
    "            values = trip_df[\"obd_engine_load\"].tolist()\n",
    "            valid_range = (0, 100)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        pid_data_list.append({\n",
    "            \"pid\": pid,\n",
    "            \"values\": values,\n",
    "            \"strategy\": strategy,\n",
    "            \"valid_range\": valid_range\n",
    "        })\n",
    "\n",
    "    # Calculate average latency\n",
    "    latency_ms = (trip_df[\"ts_uploaded\"] - trip_df[\"ts_recorded\"]).dt.total_seconds().mean() * 1000\n",
    "\n",
    "    # Extract global config params\n",
    "    send_period = config.get(\"send_period\", 60)\n",
    "    min_time = config.get(\"min_time\", 60)\n",
    "    min_saved = config.get(\"min_saved_records\", 2)\n",
    "\n",
    "    # Compute full reward with latency, PIDs, and global config\n",
    "    reward, breakdown = compute_reward_with_details(\n",
    "        latency_ms,\n",
    "        pid_data_list,\n",
    "        send_period=send_period,\n",
    "        min_time=min_time,\n",
    "        min_saved=min_saved\n",
    "    )\n",
    "\n",
    "    if trace:\n",
    "        print(f\"Reward: {reward:.2f}, Latency: {latency_ms:.1f} ms, Breakdown: {breakdown}\")\n",
    "\n",
    "    return reward, breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d06c990",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_q_agent(\n",
    "    agent,\n",
    "    baseline_config_vector,\n",
    "    start_ts,\n",
    "    end_ts,\n",
    "    obd_csv_path,\n",
    "    episodes=50,\n",
    "    trace=True\n",
    "):\n",
    "    import pandas as pd\n",
    "    from dateutil.parser import isoparse\n",
    "\n",
    "    # Load and filter OBD data\n",
    "    df = pd.read_csv(obd_csv_path)\n",
    "    df[\"ts_recorded\"] = pd.to_datetime(df[\"ts_recorded\"])\n",
    "    start_dt, end_dt = pd.to_datetime(start_ts), pd.to_datetime(end_ts)\n",
    "    df_trip = df[(df[\"ts_recorded\"] >= start_dt) & (df[\"ts_recorded\"] <= end_dt)].copy()\n",
    "\n",
    "    state = baseline_config_vector.copy()\n",
    "    rewards_per_episode = []\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        action = agent.select_action(state)\n",
    "        next_state, trace_info = apply_action(state, action, trace=False, baseline=baseline_config_vector)\n",
    "\n",
    "        try:\n",
    "            reward, breakdown = run_vehicle_sim(\n",
    "                config_vector=next_state,\n",
    "                trip_df=df_trip,\n",
    "                trace=False\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"[Episode {ep}] Simulation failed: {e}\")\n",
    "            reward = -1.0\n",
    "            breakdown = {\n",
    "                \"latency_ms\": float(\"nan\"),\n",
    "                \"latency_score\": -0.5,\n",
    "                \"pid_scores\": [],\n",
    "                \"total_pid_score\": 0.0\n",
    "            }\n",
    "\n",
    "        agent.update(state, action, reward, next_state)\n",
    "        rewards_per_episode.append(reward)\n",
    "        state = next_state\n",
    "\n",
    "        if trace:\n",
    "            print(f\"[Episode {ep}] Latency: {breakdown['latency_ms']:.0f} ms\")\n",
    "            print(f\"[Episode {ep}] Action: {action}, Reward: {reward:.3f}\")\n",
    "\n",
    "        agent.decay_epsilon()\n",
    "\n",
    "    return rewards_per_episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13e8b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "for num in range(0, 5):\n",
    "\n",
    "    # Load config and encode vector\n",
    "    with open(f'./device_config/config_{num}.json', \"r\") as json_file:\n",
    "        config_ = json.load(json_file)\n",
    "        c_vector = encode_config(config_)\n",
    "        print(f\"Config vector {num}:\", c_vector)\n",
    "\n",
    "    start_time = config_[\"test_metadata\"][0][\"ts_start\"]\n",
    "    end_time = config_[\"test_metadata\"][0][\"ts_end\"]\n",
    "    obd_csv_path = './es_events/obd_data_main.csv'\n",
    "\n",
    "    # Instantiate QAgent\n",
    "    agent = QAgent(\n",
    "        action_space_size=len(ACTIONS),   \n",
    "        epsilon = 0.4,\n",
    "        alpha   = 0.6,\n",
    "        gamma   = 0.85\n",
    "    )\n",
    "\n",
    "    # Train and save rewards to file\n",
    "    print(f\"Running training for config_{num}\")\n",
    "    rewards = train_q_agent(\n",
    "        agent=agent,\n",
    "        baseline_config_vector=c_vector,\n",
    "        start_ts=start_time,\n",
    "        end_ts=end_time,\n",
    "        obd_csv_path=obd_csv_path,\n",
    "        episodes=100,\n",
    "        trace=True\n",
    "    )\n",
    "\n",
    "    # Save rewards to text file\n",
    "    with open(f\"config_{num}_results.txt\", \"w\") as f:\n",
    "        for i, r in enumerate(rewards):\n",
    "            f.write(f\"Episode {i}: Reward {r:.3f}\\n\")\n",
    "\n",
    "    # Save reward plot to image\n",
    "    plt.figure()\n",
    "    plt.plot(rewards)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.title(f\"Q-Learning Reward Progress (config_{num})\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f\"config_{num}_results.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # Save Q-table to JSON file\n",
    "    agent.save_q_table(f\"q_table_config{num}_run1.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73d5bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for state, q_values in agent.q_table.items():\n",
    "#     best_action = q_values.index(max(q_values))\n",
    "#     print(f\"State: {state} → Best action: {best_action}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373eab85",
   "metadata": {},
   "source": [
    "### Unit tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28b0112",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Helper functions\n",
    "\n",
    "def generate_random_config() -> dict:\n",
    "    config = {\n",
    "        \"obd_parameters\": [],\n",
    "        \"global_settings\": {}\n",
    "    }\n",
    "\n",
    "    for pid in ACTIVE_PIDS:\n",
    "        enabled = random.choice([True, False])\n",
    "        strategy = random.choice(STRATEGY_LIST)\n",
    "        config[\"obd_parameters\"].append({\n",
    "            \"pid\": pid,\n",
    "            \"enabled\": enabled,\n",
    "            \"strategy\": strategy\n",
    "        })\n",
    "\n",
    "    config[\"global_settings\"] = {\n",
    "        \"min_time\": random.choice([5, 10, 30, 60, 120, 300]),\n",
    "        \"send_period\": random.choice([10, 30, 60, 120, 300]),\n",
    "        \"min_saved_records\": random.randint(1, 10)\n",
    "    }\n",
    "\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160362d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise StopIteration(\"Manual testing block below. Execution stopped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d3d8d8",
   "metadata": {},
   "source": [
    "#### Encode / decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff4d0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"obd_parameters\": [\n",
    "        {\"pid\": 12, \"enabled\": True, \"strategy\": \"on_change\"},\n",
    "        {\"pid\": 13, \"enabled\": True, \"strategy\": \"monitoring\"},\n",
    "        {\"pid\": 39, \"enabled\": False, \"strategy\": \"hysteresis\"},\n",
    "    ],\n",
    "    \"global_settings\": {\n",
    "        \"min_time\": 10,\n",
    "        \"send_period\": 60,\n",
    "        \"min_saved_records\": 1\n",
    "    }\n",
    "}\n",
    "\n",
    "vec = encode_config(config)\n",
    "decoded = decode_config(vec)\n",
    "assert config == decoded  # Should pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c47793",
   "metadata": {},
   "source": [
    "#### Apply action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04989f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_apply_action_on_all_ids():\n",
    "    base_config = generate_random_config()\n",
    "    baseline_vector = encode_config(base_config)\n",
    "    print(\"Base Config:\", decode_config(baseline_vector))\n",
    "\n",
    "    n_pids = len(ACTIVE_PIDS)\n",
    "    total_actions = len(ACTIONS)\n",
    "\n",
    "    for action_id in range(total_actions):\n",
    "        print(f\"\\n--- Testing action {action_id} ---\")\n",
    "        new_vector, trace = apply_action(baseline_vector, action_id, trace=True, baseline=baseline_vector)\n",
    "\n",
    "        # Assert same length\n",
    "        assert len(new_vector) == len(baseline_vector), f\"Vector length changed for action {action_id}\"\n",
    "\n",
    "        # Assert valid strategy index\n",
    "        for i in range(n_pids):\n",
    "            strategy_idx = new_vector[i * 3 + 2]\n",
    "            assert 0 <= strategy_idx < len(STRATEGY_LIST), f\"Invalid strategy index {strategy_idx} after action {action_id}\"\n",
    "\n",
    "        # Global settings sanity check\n",
    "        g_base = n_pids * 3\n",
    "        assert new_vector[g_base] >= 1, \"min_time below 1\"\n",
    "        assert new_vector[g_base + 1] >= 1, \"send_period below 1\"\n",
    "        assert 1 <= new_vector[g_base + 2] <= 10, \"min_saved_records out of bounds\"\n",
    "\n",
    "        print(\"Trace:\", trace)\n",
    "        print(\"✅ Passed all assertions for action\", action_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca352ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_apply_action_on_all_ids()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3daff572",
   "metadata": {},
   "source": [
    "#### Q-Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aedb1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_qagent_basic():\n",
    "    agent = QAgent(action_space_size=13, epsilon=0.0)  # deterministic\n",
    "    state = encode_config({\n",
    "        \"obd_parameters\": [\n",
    "            {\"pid\": 12, \"enabled\": True, \"strategy\": \"on_change\"},\n",
    "            {\"pid\": 13, \"enabled\": True, \"strategy\": \"on_change\"},\n",
    "            {\"pid\": 39, \"enabled\": True, \"strategy\": \"on_change\"}\n",
    "        ],\n",
    "        \"global_settings\": {\"min_time\": 10, \"send_period\": 60, \"min_saved_records\": 1}\n",
    "    })\n",
    "\n",
    "    action = agent.select_action(state)\n",
    "    next_state, _ = apply_action(state, action, baseline=state)\n",
    "    reward = 2.5\n",
    "    agent.update(state, action, reward, next_state)\n",
    "\n",
    "    # Assert Q-value updated\n",
    "    state_key = tuple(state)\n",
    "    assert action < len(agent.q_table[state_key]), \"Q-table entry missing\"\n",
    "    q_value = agent.q_table[state_key][action]\n",
    "    assert q_value != 0.0, \"Q-value not updated\"\n",
    "    print(f\"✅ Q-table updated: Q[state][{action}] = {q_value:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70372d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_qagent_basic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83df0d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_qagent_training():\n",
    "    baseline_config = {\n",
    "        \"obd_parameters\": [\n",
    "            {\"pid\": 12, \"enabled\": True, \"strategy\": \"monitoring\"},\n",
    "            {\"pid\": 13, \"enabled\": True, \"strategy\": \"monitoring\"},\n",
    "            {\"pid\": 39, \"enabled\": True, \"strategy\": \"monitoring\"}\n",
    "        ],\n",
    "        \"global_settings\": {\"min_time\": 300, \"send_period\": 120, \"min_saved_records\": 1}\n",
    "    }\n",
    "\n",
    "    baseline_vector = encode_config(baseline_config)\n",
    "    agent = QAgent(action_space_size=13, epsilon=0.3)\n",
    "\n",
    "    rewards = train_q_agent(\n",
    "        agent,\n",
    "        baseline_vector,\n",
    "        start_ts=\"2025-05-16T06:40:38Z\",\n",
    "        end_ts=\"2025-05-17T23:59:00Z\"  ,\n",
    "        freq_csv_path=\"../../data_proc/csv_data/qa_device/frequencies.csv\",\n",
    "        obd_csv_path=\"../../data_proc/csv_data/qa_device/obd_export.csv\",\n",
    "        episodes=5,\n",
    "        trace=True\n",
    "    )\n",
    "\n",
    "    assert len(rewards) == 5, \"Incorrect number of training episodes\"\n",
    "    assert all(isinstance(r, (float, int)) for r in rewards), \"Non-numeric reward detected\"\n",
    "    print(\"✅ Training rewards:\", rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ffeb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_qagent_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c28be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training\n",
    "\n",
    "baseline_config = {\n",
    "    \"obd_parameters\": [\n",
    "        {\"pid\": 12, \"enabled\": True, \"strategy\": \"monitoring\"},\n",
    "        {\"pid\": 13, \"enabled\": True, \"strategy\": \"monitoring\"},\n",
    "        {\"pid\": 39, \"enabled\": True, \"strategy\": \"monitoring\"}\n",
    "    ],\n",
    "    \"global_settings\": {\"min_time\": 300, \"send_period\": 120, \"min_saved_records\": 1}\n",
    "}\n",
    "\n",
    "baseline_vector = encode_config(baseline_config)\n",
    "\n",
    "agent = QAgent(action_space_size=13, epsilon=0.3)\n",
    "rewards = train_q_agent(\n",
    "    agent,\n",
    "    baseline_vector,\n",
    "    start_ts=\"2025-05-23T13:55:00Z\",\n",
    "    end_ts=\"2025-05-23T14:07:00Z\",\n",
    "    freq_csv_path=\"frequencies.csv\",\n",
    "    obd_csv_path=\"obd_export.csv\"\n",
    ")\n",
    "\n",
    "plot_rewards(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883353b0",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27132366",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Implementation references**\n",
    "\n",
    "- OpenAI Spinning Up: https://spinningup.openai.com\n",
    "Although it focuses more on policy-gradient methods, it gives good context on where Q-learning fits in the broader RL ecosystem.\n",
    "\n",
    "- RL Course by David Silver (DeepMind)\n",
    "Lectures 4–6 cover model-free methods, including Q-Learning.\n",
    "\n",
    "- Towards Data Science\n",
    "https://towardsdatascience.com/reinforcement-learning-explained-visually-part-4-q-learning-step-by-step-b65efb731d3e/\n",
    "\n",
    "\n",
    "\n",
    "**Academic references**\n",
    "\n",
    "1. Watkins, C.J.C.H., & Dayan, P. (1992)\n",
    "   Q-learning: https://link.springer.com/article/10.1007/BF00992698\n",
    "\n",
    "   \n",
    "2. Sutton, R. S., & Barto, A. G. (2018)\n",
    "    Reinforcement Learning: An Introduction (2nd Edition)\n",
    "    Chapter 6 covers Q-Learning in depth.\n",
    "    http://incompleteideas.net/book/the-book-2nd.html "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aau",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

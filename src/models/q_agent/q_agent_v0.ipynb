{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4db09d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import random\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316ccab3",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9687088",
   "metadata": {},
   "source": [
    "**Baseline config** initial state:  (used in production)   \n",
    "3 PIDs (engine load, rpm, speed) set to monitoring, with 300 s Min Period (distance, angle, speed delta are not used), min saved records = 1, send period = 120.\n",
    "\n",
    "**Worst config**  initial state (from my own experience)  \n",
    "3 PIDs (engine load, rpm, speed) set to on_change, with 1 s Min Period (distance, angle, speed delta are not used), min saved records = 1, send period = 1.\n",
    "\n",
    "**Test conditions**\n",
    "1. \"Vehicle trip\" (12 minutes vehicle trip with values increasing monotonically)\n",
    "2.  Sleep, battery, GPS precision and network mode (home, roaming, unkown) are assumed optimal. \n",
    "\n",
    "**State representation**  \n",
    "1. $p_{i_0}$----PID (integer identifier for PID, e.g. 12 = RPM)\n",
    "2. $p_{i_1}$----change data acquisition strategy for PID $i$  *(off, monitoring, on_change, hysteresis, delta_change)*  \n",
    "3. $d_0$------increase/decrease min_time  (time the device lets pass before querying PID values)     *(seconds, 0 disables it)*  \n",
    "4. $d_1$------increase/decrease min_saved (number of records the device will accumulate before sending) *(int, 0 disables it)*  \n",
    "5. $d_2$------increase/decrease send_time  (time the device lets pass before attempting to send a new record)  *(seconds, 0 disables it)*  \n",
    " \n",
    "\n",
    "For example, considering 3 different PIDs a state $S$ is defined like: \n",
    "\n",
    "$S = (p_{10}, p_{11}, p_{20}, p_{21}, p_{30}, p_{31}, d_0, d_1, d_2)$\n",
    "\n",
    "So for 3 PIDs, the $S$ vector is of size $9$, and in general:  $2n + 3$\n",
    "\n",
    "$S = (p_{10}, p_{11}, p_{20}, p_{21}, ..., p_{n0}, p_{n1}, d_0, d_1, d_2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0a4641",
   "metadata": {},
   "source": [
    "### PID configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdf3a675",
   "metadata": {},
   "outputs": [],
   "source": [
    "STRATEGY_MAP = {\n",
    "    \"off\": 0,\n",
    "    \"monitoring\": 1,\n",
    "    \"on_change\": 2,\n",
    "    \"hysteresis\": 3,\n",
    "    \"on_delta_change\": 4 # on_enter, on_exit, on_both are omitted to simplify the model.\n",
    "}\n",
    "\n",
    "REVERSE_STRATEGY_MAP = {v: k for k, v in STRATEGY_MAP.items()}\n",
    "STRATEGY_LIST = list(STRATEGY_MAP.keys())\n",
    "\n",
    "ACTIVE_PIDS = [12, 13, 39]  # add all necessary PIDs\n",
    "n_pids = len(ACTIVE_PIDS)\n",
    "\n",
    "PID_PRECISION = {\n",
    "    12: 100,\n",
    "    13: 5,\n",
    "    39: 5,\n",
    "    28: 2,\n",
    "    38: 3,\n",
    "    31: 3\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f36e25",
   "metadata": {},
   "source": [
    "### State encoder / decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdf41bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_config(config: dict) -> list:\n",
    "    \"\"\"\n",
    "    Converts a human-readable config dict into a state vector.\n",
    "    Each PID is encoded as [pid, strategy_idx], where strategy_idx includes 'off'.\n",
    "    \"\"\"\n",
    "    vector = []\n",
    "\n",
    "    # Encode OBD parameters\n",
    "    for param in config.get(\"obd_parameters\", []):\n",
    "        pid = param.get(\"pid\", 0)\n",
    "        strategy_idx = STRATEGY_MAP.get(param.get(\"strategy\", \"monitoring\"), 1)\n",
    "        vector.extend([pid, strategy_idx])\n",
    "\n",
    "    # Global settings\n",
    "    global_ = config.get(\"global_settings\", {})\n",
    "    vector.extend([\n",
    "        global_.get(\"min_time\", 10),\n",
    "        global_.get(\"send_period\", 10),\n",
    "        global_.get(\"min_saved_records\", 1)\n",
    "    ])\n",
    "\n",
    "    return vector\n",
    "\n",
    "\n",
    "def decode_config(vector: list) -> dict:\n",
    "    \"\"\"\n",
    "    Converts a state vector into a human-readable config dict.\n",
    "    Assumes 2 elements per PID: [pid, strategy_idx], followed by 3 global settings.\n",
    "    \"\"\"\n",
    "    config = {\"obd_parameters\": [], \"global_settings\": {}}\n",
    "\n",
    "    # Number of PIDs can be inferred\n",
    "    pid_section_length = len(vector) - 3  # last 3 are global\n",
    "    assert pid_section_length % 2 == 0, \"Invalid config vector length\"\n",
    "\n",
    "    n_pids = pid_section_length // 2\n",
    "\n",
    "    # Decode OBD parameter block\n",
    "    for i in range(n_pids):\n",
    "        base = i * 2\n",
    "        pid, strategy_idx = vector[base:base + 2]\n",
    "        config[\"obd_parameters\"].append({\n",
    "            \"pid\": pid,\n",
    "            \"enabled\": strategy_idx != 0,  # 'off' means not enabled\n",
    "            \"strategy\": REVERSE_STRATEGY_MAP.get(strategy_idx, \"monitoring\")\n",
    "        })\n",
    "\n",
    "    # Decode global settings\n",
    "    g_base = n_pids * 2\n",
    "    config[\"global_settings\"] = {\n",
    "        \"min_time\": vector[g_base],\n",
    "        \"send_period\": vector[g_base + 1],\n",
    "        \"min_saved_records\": vector[g_base + 2]\n",
    "    }\n",
    "\n",
    "    return config\n",
    "\n",
    "\n",
    "def load_config(num):\n",
    "    with open(f'./device_config/config_{num}.json', \"r\") as json_file:\n",
    "        config_ = json.load(json_file)\n",
    "        c_vector = encode_config(config_)\n",
    "        print(f\"Config vector {num} loaded:\", c_vector)\n",
    "        return config_, c_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0300928",
   "metadata": {},
   "source": [
    "### Actions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4d2b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTIONS = []\n",
    "\n",
    "# Strategy change for enabled PIDs\n",
    "for pid in ACTIVE_PIDS:\n",
    "    ACTIONS.append({\"type\": \"cycle_strategy\", \"pid\": pid}) \n",
    "\n",
    "# Adjust min_time in reasonable steps\n",
    "for delta in [-30, -15, -5, -1, 1, 5, 15, 30]:\n",
    "    ACTIONS.append({\"type\": \"adjust\", \"param\": \"min_time\", \"delta\": delta})\n",
    "\n",
    "# Adjust send_period in similar steps\n",
    "for delta in [-30, -15, -5, -1, 1, 5, 15, 30]:\n",
    "    ACTIONS.append({\"type\": \"adjust\", \"param\": \"send_period\", \"delta\": delta})\n",
    "\n",
    "# Cycle min_saved_records 1 → 2 → 3 → 4 → 1\n",
    "ACTIONS.append({\"type\": \"cycle\", \"param\": \"min_saved_records\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "804282f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pids = len(ACTIVE_PIDS)\n",
    "g_base = n_pids * 2  # index where global settings start\n",
    "\n",
    "def apply_action(state: list[int], action_id: int, \n",
    "                 trace: bool = True) -> tuple[list[int], dict]:\n",
    "    \"\"\"\n",
    "    Applies a single action to the config vector and returns (new_state, trace_dict).\n",
    "    The state layout is: [pid, strategy_idx] * N + [min_time, send_period, min_saved_records]\n",
    "    \"\"\"\n",
    "    new_vector = deepcopy(state)\n",
    "    trace_info = {}\n",
    "\n",
    "    n_pids = len(ACTIVE_PIDS)\n",
    "    g_base = n_pids * 2  # 2 elements per PID now\n",
    "\n",
    "    def log(msg):\n",
    "        if trace:\n",
    "            print(f\"[apply_action] {msg}\")\n",
    "        trace_info[\"change\"] = msg\n",
    "\n",
    "    # PID strategy cycle: action 0 to N-1\n",
    "    if action_id < n_pids:\n",
    "        idx = action_id\n",
    "        base = idx * 2\n",
    "        pid = new_vector[base]\n",
    "        strategy = new_vector[base + 1]\n",
    "\n",
    "        if strategy == 0:\n",
    "            log(f\"PID {pid} is off. Strategy unchanged.\")\n",
    "        else:\n",
    "            next_strategy = (strategy - 1 + 1) % 4 + 1  # 1 → 2 → 3 → 4 → 1\n",
    "            new_vector[base + 1] = next_strategy\n",
    "            log(f\"PID {pid} strategy: {STRATEGY_LIST[strategy]} → {STRATEGY_LIST[next_strategy]}\")\n",
    "        return new_vector, trace_info\n",
    "\n",
    "    # Global actions\n",
    "    global_action_id = action_id - n_pids\n",
    "\n",
    "    # Adjust steps\n",
    "    global_adjustments = {\n",
    "        \"min_time\": {\"index\": g_base, \"bounds\": (0, 300)},\n",
    "        \"send_period\": {\"index\": g_base + 1, \"bounds\": (0, 300)},\n",
    "    }\n",
    "\n",
    "    # Cycle min_saved_records separately\n",
    "    if global_action_id >= len([-30, -15, -5, -1, 1, 5, 15, 30]) * 2:\n",
    "        index = g_base + 2\n",
    "        old_val = new_vector[index]\n",
    "        new_vector[index] = (old_val % 4) + 1\n",
    "        log(f\"min_saved_records cycled from {old_val} → {new_vector[index]}\")\n",
    "        return new_vector, trace_info\n",
    "\n",
    "    # For min_time and send_period\n",
    "    deltas = [-30, -15, -5, -1, 1, 5, 15, 30]\n",
    "    n_deltas = len(deltas)\n",
    "\n",
    "    if global_action_id < n_deltas:\n",
    "        label = \"min_time\"\n",
    "        delta = deltas[global_action_id]\n",
    "    else:\n",
    "        label = \"send_period\"\n",
    "        delta = deltas[global_action_id - n_deltas]\n",
    "\n",
    "    index = global_adjustments[label][\"index\"]\n",
    "    min_val, max_val = global_adjustments[label][\"bounds\"]\n",
    "    old_val = new_vector[index]\n",
    "    new_val = max(min(old_val + delta, max_val), min_val)\n",
    "    new_vector[index] = new_val\n",
    "    log(f\"{label} changed from {old_val} → {new_val}\")\n",
    "    return new_vector, trace_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c96c8ac",
   "metadata": {},
   "source": [
    "### Reward function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b52f8ac",
   "metadata": {},
   "source": [
    "reward = latency_score + sum(pid_scores)\n",
    "\n",
    "For example:    \n",
    "    latency_ms = 1850\n",
    "```json\n",
    "    pid_data_list = [\n",
    "        {\n",
    "            \"pid\": 12,\n",
    "            \"values\": [1500, 1500, 1500],\n",
    "            \"strategy\": \"on_change\",\n",
    "            \"precision\": 100,\n",
    "            \"valid_range\": (800, 6000)\n",
    "        },\n",
    "        // rest of the PIDs\n",
    "]\n",
    "```\n",
    "\n",
    "**Precision**  \n",
    "This defines how much a value must change to be considered “meaningful.”\n",
    "Even if all values are valid, we may want to ignore tiny fluctuations (e.g., RPM changes by 1 unit) and log only useful variation (e.g., ±100 RPM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "037d3c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def latency_score(latency_ms: float) -> float:\n",
    "    \"\"\"\n",
    "    Reward stable and reasonable latency.\n",
    "    - Under 5s: full reward\n",
    "    - Between 5s–20s: moderate reward\n",
    "    - 20s–30s: penalize\n",
    "    - Over 30s: heavy penalty\n",
    "    \"\"\"\n",
    "    if latency_ms <= 5000:\n",
    "        return 1.0\n",
    "    elif latency_ms <= 20000:\n",
    "        return 0.6\n",
    "    elif latency_ms <= 30000:\n",
    "        return -0.3\n",
    "    else:\n",
    "        return -1.0\n",
    "\n",
    "\n",
    "def global_config_penalty(send_period: int, min_time: int, min_saved: int) -> float:\n",
    "    \"\"\"\n",
    "    Penalizes or rewards based on how close parameters are to ideal ranges.\n",
    "    - Ideal:\n",
    "        send_period ≈ 60–120\n",
    "        min_time ≈ 60–300\n",
    "        min_saved ≈ 2–4\n",
    "    \"\"\"\n",
    "    score = 0.0\n",
    "\n",
    "    # --- min_saved_records ---\n",
    "    if min_saved < 2:\n",
    "        score -= 0.4  # too aggressive\n",
    "    elif min_saved > 4:\n",
    "        score -= 0.5  # queues too much\n",
    "    else:\n",
    "        score += 0.1  # ideal\n",
    "\n",
    "    # --- send_period ---\n",
    "    if send_period < 10:\n",
    "        score -= 0.6\n",
    "    elif send_period < 30:\n",
    "        score -= 0.3\n",
    "    elif send_period > 150:\n",
    "        score -= 0.4\n",
    "    elif 60 <= send_period <= 120:\n",
    "        score += 0.2  # optimal\n",
    "    else:\n",
    "        score += 0.05  # acceptable\n",
    "\n",
    "    # --- min_time ---\n",
    "    if min_time < 10:\n",
    "        score -= 0.3\n",
    "    elif min_time > 600:\n",
    "        score -= 0.2\n",
    "    elif 60 <= min_time <= 300:\n",
    "        score += 0.2\n",
    "    else:\n",
    "        score += 0.05\n",
    "\n",
    "    return round(score, 3)\n",
    "    \n",
    "\n",
    "def compute_pid_scores(pid_data_list: list[dict], send_period: int) -> list[dict]:\n",
    "    scores = []\n",
    "    for entry in pid_data_list:\n",
    "        strategy = entry[\"strategy\"]\n",
    "        if strategy == \"off\":\n",
    "            continue  # ← Skip unused PIDs\n",
    "\n",
    "        score = data_quality_score(\n",
    "            values=entry[\"values\"],\n",
    "            strategy=strategy,\n",
    "            precision=entry[\"precision\"],\n",
    "            valid_range=entry[\"valid_range\"],\n",
    "            send_period=send_period\n",
    "        )\n",
    "        scores.append({\n",
    "            \"pid\": entry[\"pid\"],\n",
    "            \"score\": round(score, 3)\n",
    "        })\n",
    "    return scores\n",
    "\n",
    "\n",
    "\n",
    "def data_quality_score(\n",
    "    values: list[float],\n",
    "    strategy: str,\n",
    "    precision: float,\n",
    "    valid_range: tuple[float, float],\n",
    "    send_period: int,\n",
    ") -> float:\n",
    "    \"\"\"Scores PID quality: range check + smooth variation proportional to sampling interval.\"\"\"\n",
    "    if not values:\n",
    "        return -1.0\n",
    "\n",
    "    # 1. All values must be in valid range and non-zero\n",
    "    if not all(valid_range[0] <= v <= valid_range[1] for v in values):\n",
    "        return -1.0\n",
    "    if all(v == 0 for v in values):\n",
    "        return -1.0\n",
    "\n",
    "    # 2. Variation scaled to time gap\n",
    "    variation = max(values) - min(values)\n",
    "    allowed_variation = precision * (send_period / 10)  # allow bigger jumps for longer intervals\n",
    "    significant = variation >= allowed_variation\n",
    "\n",
    "    # 3. Strategy reward logic\n",
    "    match strategy:\n",
    "        case \"on_change\":\n",
    "            return 1.0 if not significant else 0.5\n",
    "        case \"on_delta_change\":\n",
    "            return 1.0 if significant else -0.5\n",
    "        case \"hysteresis\":\n",
    "            return 0.8 if significant else 0.2\n",
    "        case \"monitoring\":\n",
    "            return 0.6 if significant else -0.2\n",
    "\n",
    "    return 0.0  # fallback\n",
    "\n",
    "\n",
    "def compute_average_quality(pid_data_list: list[dict], send_period: int) -> float:\n",
    "    scores = []\n",
    "    for entry in pid_data_list:\n",
    "        strategy = entry[\"strategy\"]\n",
    "        if strategy == \"off\":\n",
    "            continue\n",
    "\n",
    "        score = data_quality_score(\n",
    "            values=entry[\"values\"],\n",
    "            strategy=strategy,\n",
    "            precision=entry[\"precision\"],\n",
    "            valid_range=entry[\"valid_range\"],\n",
    "            send_period=send_period\n",
    "        )\n",
    "        scores.append(score)\n",
    "\n",
    "    return round(sum(scores), 3) if scores else 0.0\n",
    "\n",
    "\n",
    "def clip_reward(score, min_val=-1.0, max_val=3.0):\n",
    "    \"\"\"Optional clipping to stabilize training and limit outliers.\"\"\"\n",
    "    return max(min(score, max_val), min_val)\n",
    "\n",
    "\n",
    "def compute_reward(latency_ms: int, pid_data_list: list[dict], send_period: int, min_time: int, min_saved: int) -> float:\n",
    "    latency = latency_score(latency_ms)\n",
    "    quality_sum = compute_average_quality(pid_data_list, send_period)\n",
    "    penalty = global_config_penalty(send_period, min_time, min_saved)\n",
    "    return round(latency + quality_sum + penalty, 3)\n",
    "\n",
    "\n",
    "def compute_reward_with_details(latency_ms: int, pid_data_list: list[dict], send_period: int, \n",
    "                                min_time: int, min_saved: int) -> tuple[float, dict]:\n",
    "    for entry in pid_data_list:\n",
    "        entry[\"precision\"] = PID_PRECISION.get(entry[\"pid\"], 1)\n",
    "\n",
    "    latency = latency_score(latency_ms)\n",
    "    pid_scores = compute_pid_scores(pid_data_list, send_period)\n",
    "    total_pid_score = sum(entry[\"score\"] for entry in pid_scores)\n",
    "    config_penalty = global_config_penalty(send_period, min_time, min_saved)\n",
    "\n",
    "    total_reward = round(latency + total_pid_score + config_penalty, 3)\n",
    "\n",
    "    breakdown = {\n",
    "        \"latency_ms\": latency_ms,\n",
    "        \"latency_score\": round(latency, 3),\n",
    "        \"pid_scores\": pid_scores,\n",
    "        \"total_pid_score\": round(total_pid_score, 3),\n",
    "        \"global_config_penalty\": config_penalty,\n",
    "    }\n",
    "\n",
    "    return total_reward, breakdown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1bf755ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flag_latency_outliers(freq_df: pd.DataFrame, threshold_ms: int = 7_200_000) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds a boolean column 'is_latency_outlier' to the input DataFrame,\n",
    "    where True indicates latency greater than the given threshold (default: 2 hours).\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert timestamps\n",
    "    freq_df[\"ts_recorded\"] = pd.to_datetime(freq_df[\"ts_recorded\"])\n",
    "    freq_df[\"ts_uploaded\"] = pd.to_datetime(freq_df[\"ts_uploaded\"])\n",
    "\n",
    "    # Compute latency in milliseconds\n",
    "    freq_df[\"latency_ms\"] = (freq_df[\"ts_uploaded\"] - freq_df[\"ts_recorded\"]).dt.total_seconds() * 1000\n",
    "\n",
    "    # Flag all rows with latency above the threshold\n",
    "    freq_df[\"is_latency_outlier\"] = freq_df[\"latency_ms\"] > threshold_ms\n",
    "\n",
    "    return freq_df\n",
    "\n",
    "\n",
    "def get_latency(start_ts, end_ts, method='median'):\n",
    "    freq_df = pd.read_csv('../../data_proc/csv_data/qa_device/frequencies.csv', low_memory=False)\n",
    "    freq_df.drop('Unnamed: 0', inplace=True, axis=1)\n",
    "\n",
    "    # Parse timestamps\n",
    "    freq_df[\"ts_recorded\"] = pd.to_datetime(freq_df[\"ts_recorded\"])\n",
    "    freq_df[\"ts_uploaded\"] = pd.to_datetime(freq_df[\"ts_uploaded\"])\n",
    "\n",
    "    # Filter within time range\n",
    "    mask = (freq_df[\"ts_recorded\"] >= pd.to_datetime(start_ts)) & (freq_df[\"ts_recorded\"] <= pd.to_datetime(end_ts))\n",
    "    filtered_df = freq_df[mask].copy()\n",
    "\n",
    "    if filtered_df.empty:\n",
    "        return None  # or raise an exception if preferred\n",
    "\n",
    "    # Compute latency in ms\n",
    "    filtered_df[\"latency_ms\"] = (filtered_df[\"ts_uploaded\"] - filtered_df[\"ts_recorded\"]).dt.total_seconds() * 1000\n",
    "\n",
    "    if method == 'median':\n",
    "        return filtered_df[\"latency_ms\"].median()\n",
    "    elif method == 'mean':\n",
    "        return filtered_df[\"latency_ms\"].mean()\n",
    "    else:\n",
    "        raise ValueError(\"method must be 'median' or 'mean'\")\n",
    "\n",
    "\n",
    "# Define a field-to-PID mapping\n",
    "FIELD_TO_PID = {\n",
    "    \"obd.rpm.value\": 12,\n",
    "    \"obd.speed.value\": 13,\n",
    "    \"obd.fuel_level.value\": 28,\n",
    "    \"obd.coolant_temp.value\": 38,\n",
    "    \"obd.engine_load.value\": 39,\n",
    "    \"obd.intake_temp.value\": 20,\n",
    "    \"obd.maf.value\": 21,\n",
    "    \"obd.throttle_pos.value\": 41,\n",
    "    \"obd.ambient_air_temp.value\": 131,\n",
    "    \"obd.distance_since_codes_clear.value\": 31,\n",
    "    \"obd.time_since_codes_cleared.value\": 47,\n",
    "    # Add more as needed...\n",
    "}\n",
    "\n",
    "\n",
    "def extract_pid_statistics(obd_df: pd.DataFrame, start_ts: str, end_ts: str) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Extracts per-PID reward inputs (PID ID, values, precision, valid range)\n",
    "    from an obd_export dataframe filtered by timestamp range.\n",
    "    Returns a list of dictionaries ready for reward scoring.\n",
    "    \"\"\"\n",
    "    # Filter by time window\n",
    "    obd_df[\"@ts\"] = pd.to_datetime(obd_df[\"@ts\"])\n",
    "    mask = (obd_df[\"@ts\"] >= pd.to_datetime(start_ts)) & (obd_df[\"@ts\"] <= pd.to_datetime(end_ts))\n",
    "    obd_df = obd_df[mask].copy()\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for field, pid in FIELD_TO_PID.items():\n",
    "        if field not in obd_df.columns:\n",
    "            continue\n",
    "\n",
    "        values = obd_df[field].dropna().astype(float).tolist()\n",
    "        if not values:\n",
    "            continue\n",
    "\n",
    "        vmin, vmax = min(values), max(values)\n",
    "        vrange = vmax - vmin\n",
    "\n",
    "        precision = round(vrange * 0.1, 3) if vrange > 0 else 1.0\n",
    "        valid_range = (vmin - vrange * 0.1, vmax + vrange * 0.1)\n",
    "\n",
    "        results.append({\n",
    "            \"pid\": pid,\n",
    "            \"values\": values,\n",
    "            \"precision\": precision,\n",
    "            \"valid_range\": valid_range\n",
    "        })\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d7fcb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_pid_statistics(pid_data_list: list[dict]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns a DataFrame summarizing each PID's stats (excluding raw values).\n",
    "    \"\"\"\n",
    "    summary = []\n",
    "\n",
    "    for entry in pid_data_list:\n",
    "        values = entry[\"values\"]\n",
    "        pid_summary = {\n",
    "            \"PID\": entry[\"pid\"],\n",
    "            \"Count\": len(values),\n",
    "            \"Min\": min(values) if values else None,\n",
    "            \"Max\": max(values) if values else None,\n",
    "            \"Precision\": PID_PRECISION.get(entry[\"pid\"], 1),\n",
    "            \"Valid Range\": entry[\"valid_range\"],\n",
    "        }\n",
    "        summary.append(pid_summary)\n",
    "\n",
    "    return pd.DataFrame(summary).sort_values(by=\"PID\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e563591f",
   "metadata": {},
   "source": [
    "### Q-Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf3979bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAgent:\n",
    "    def __init__(self, action_space_size, epsilon=0.2, alpha=0.5, gamma=0.9):\n",
    "        self.q_table = defaultdict(self.default_q_values)\n",
    "        self.epsilon = epsilon  # exploration rate NOTE tune better the parameters\n",
    "        self.alpha = alpha      # learning rate\n",
    "        self.gamma = gamma      # discount factor\n",
    "        self.action_space_size = action_space_size\n",
    "\n",
    "    def default_q_values(self):\n",
    "        return [0.0] * len(ACTIONS)\n",
    "\n",
    "    def select_action(self, state_vector: list[int]) -> int:\n",
    "        state_key = tuple(state_vector)\n",
    "\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, self.action_space_size - 1)  # explore\n",
    "        else:\n",
    "            q_values = self.q_table[state_key]\n",
    "            return int(q_values.index(max(q_values)))  # exploit\n",
    "\n",
    "    def update(self, state: list[int], action: int, reward: float, next_state: list[int]):\n",
    "        state_key = tuple(state)\n",
    "        next_state_key = tuple(next_state)\n",
    "\n",
    "        old_value = self.q_table[state_key][action]\n",
    "        next_max = max(self.q_table[next_state_key])\n",
    "\n",
    "        # Q-learning update rule\n",
    "        new_value = old_value + self.alpha * (reward + self.gamma * next_max - old_value)\n",
    "        self.q_table[state_key][action] = new_value\n",
    "\n",
    "    def decay_epsilon(self, decay_rate=0.99):\n",
    "        self.epsilon *= decay_rate\n",
    "\n",
    "    def save_q_table(self, path='q_table.json'):\n",
    "        # Convert keys to strings for JSON serialization\n",
    "        json_q = {str(k): v for k, v in self.q_table.items()}\n",
    "        with open(path, 'w') as f:\n",
    "            json.dump(json_q, f, indent=2)\n",
    "\n",
    "    def load_q_table(self, path='q_table.json'):\n",
    "        with open(path, 'r') as f:\n",
    "            json_q = json.load(f)\n",
    "        self.q_table = defaultdict(lambda: [0.0] * self.action_space_size)\n",
    "        for k, v in json_q.items():\n",
    "            self.q_table[tuple(eval(k))] = v\n",
    "\n",
    "    def get_best_config_vector(self):\n",
    "        \"\"\"\n",
    "        Returns the state (config vector) with the highest Q-value across all actions.\n",
    "        \"\"\"\n",
    "        best_state = None\n",
    "        best_value = float(\"-inf\")\n",
    "\n",
    "        for state, action_values in self.q_table.items():\n",
    "            max_q = max(action_values)\n",
    "            if max_q > best_value:\n",
    "                best_value = max_q\n",
    "                best_state = state\n",
    "\n",
    "        if best_state is None:\n",
    "            return []\n",
    "\n",
    "        return list(best_state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb272bc6",
   "metadata": {},
   "source": [
    "### Vehicle trip simulation / Q-Agent training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d695c6c",
   "metadata": {},
   "source": [
    "1. Start from a known config (state)\n",
    "2. Select an action with ε-greedy\n",
    "3. Applies it to get next_state\n",
    "4. Run run_vehicle_sim() to compute reward\n",
    "5. Update Q-table\n",
    "6. Repeat for N episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9cfa7f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_vehicle_sim(config_vector, trip_df, trace=False):\n",
    "    config = decode_config(config_vector)\n",
    "    pid_data_list = []\n",
    "\n",
    "    # Parse timestamps\n",
    "    trip_df[\"ts_recorded\"] = pd.to_datetime(trip_df[\"ts_recorded\"], utc=True, errors=\"coerce\")\n",
    "    trip_df[\"ts_uploaded\"] = pd.to_datetime(trip_df[\"ts_uploaded\"], utc=True, errors=\"coerce\")\n",
    "    trip_df = trip_df.dropna(subset=[\"ts_recorded\", \"ts_uploaded\"])\n",
    "\n",
    "    if trip_df.empty:\n",
    "        print(\"Trip dataframe was empty after filtering.\")\n",
    "        if trace:\n",
    "            print(\"[run_vehicle_sim] trip_df is EMPTY after filtering.\")\n",
    "        return -0.5, {\n",
    "            \"latency_ms\": float(\"nan\"),\n",
    "            \"latency_score\": -0.5,\n",
    "            \"pid_scores\": [],\n",
    "            \"total_pid_score\": 0.0,\n",
    "            \"global_config_score\": -1.0,\n",
    "        }\n",
    "\n",
    "    # Extract values from trip log for enabled PIDs\n",
    "    for param in config[\"obd_parameters\"]:\n",
    "        if not param[\"enabled\"]:\n",
    "            continue\n",
    "\n",
    "        pid = param[\"pid\"]\n",
    "        strategy = param[\"strategy\"]\n",
    "\n",
    "        if pid == 12:\n",
    "            values = trip_df[\"obd_rpm\"].tolist()\n",
    "            valid_range = (0, 8000)\n",
    "        elif pid == 13:\n",
    "            values = trip_df[\"obd_speed\"].tolist()\n",
    "            valid_range = (0, 250)\n",
    "        elif pid == 39:\n",
    "            values = trip_df[\"obd_engine_load\"].tolist()\n",
    "            valid_range = (0, 100)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        pid_data_list.append({\n",
    "            \"pid\": pid,\n",
    "            \"values\": values,\n",
    "            \"strategy\": strategy,\n",
    "            \"valid_range\": valid_range\n",
    "        })\n",
    "\n",
    "    # Calculate average latency\n",
    "    latency_ms = (trip_df[\"ts_uploaded\"] - trip_df[\"ts_recorded\"]).dt.total_seconds().mean() * 1000\n",
    "\n",
    "    # Extract global config params\n",
    "    send_period = config.get(\"send_period\", 60)\n",
    "    min_time = config.get(\"min_time\", 60)\n",
    "    min_saved = config.get(\"min_saved_records\", 2)\n",
    "\n",
    "    # Compute full reward with latency, PIDs, and global config\n",
    "    reward, breakdown = compute_reward_with_details(\n",
    "        latency_ms,\n",
    "        pid_data_list,\n",
    "        send_period=send_period,\n",
    "        min_time=min_time,\n",
    "        min_saved=min_saved\n",
    "    )\n",
    "\n",
    "    if trace:\n",
    "        print(f\"Reward: {reward:.2f}, Latency: {latency_ms:.1f} ms, Breakdown: {breakdown}\")\n",
    "\n",
    "    return reward, breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d06c990",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_q_agent(\n",
    "    agent,\n",
    "    baseline_config_vector,\n",
    "    start_ts,\n",
    "    end_ts,\n",
    "    obd_csv_path,\n",
    "    episodes=50,\n",
    "    trace=True\n",
    "):\n",
    "\n",
    "    # Load and filter OBD data\n",
    "    df = pd.read_csv(obd_csv_path)\n",
    "    df[\"ts_recorded\"] = pd.to_datetime(df[\"ts_recorded\"])\n",
    "    start_dt, end_dt = pd.to_datetime(start_ts), pd.to_datetime(end_ts)\n",
    "    df_trip = df[(df[\"ts_recorded\"] >= start_dt) & (df[\"ts_recorded\"] <= end_dt)].copy()\n",
    "\n",
    "    state = baseline_config_vector.copy()\n",
    "    rewards_per_episode = []\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        action = agent.select_action(state)\n",
    "        next_state, trace_info = apply_action(state, action, trace=False)\n",
    "\n",
    "        try:\n",
    "            reward, breakdown = run_vehicle_sim(\n",
    "                config_vector=next_state,\n",
    "                trip_df=df_trip,\n",
    "                trace=False\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"[Episode {ep}] Simulation failed: {e}\")\n",
    "            reward = -1.0\n",
    "            breakdown = {\n",
    "                \"latency_ms\": float(\"nan\"),\n",
    "                \"latency_score\": -0.5,\n",
    "                \"pid_scores\": [],\n",
    "                \"total_pid_score\": 0.0\n",
    "            }\n",
    "\n",
    "        agent.update(state, action, reward, next_state)\n",
    "        rewards_per_episode.append(reward)\n",
    "        state = next_state\n",
    "\n",
    "        if trace:\n",
    "            print(f\"[Episode {ep}] Latency: {breakdown['latency_ms']:.0f} ms\")\n",
    "            print(f\"[Episode {ep}] Action: {action}, Reward: {reward:.3f}\")\n",
    "\n",
    "        agent.decay_epsilon()\n",
    "\n",
    "    return rewards_per_episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d13e8b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_100episodes():\n",
    "    for num in range(0, 5):\n",
    "\n",
    "        # Load config JSON and its vector representation\n",
    "        config_, c_vector = load_config(num)        \n",
    "\n",
    "        start_time = config_[\"test_metadata\"][0][\"ts_start\"]\n",
    "        end_time = config_[\"test_metadata\"][0][\"ts_end\"]\n",
    "        obd_csv_path = './es_events/poc_and_v0_data.csv'\n",
    "\n",
    "        init_epsilon=0.3\n",
    "        init_alpha=0.7\n",
    "        init_gamma=0.5\n",
    "\n",
    "        # Instantiate QAgent\n",
    "        agent = QAgent(\n",
    "            action_space_size=len(ACTIONS),\n",
    "            epsilon=init_epsilon,\n",
    "            alpha=init_alpha,\n",
    "            gamma=init_gamma\n",
    "        )\n",
    "\n",
    "        # Train agent\n",
    "        print(f\"Running training for config_{num} with epsilon={init_epsilon}, gamma={init_gamma}, alpha={init_alpha}\")\n",
    "        rewards = train_q_agent(\n",
    "            agent=agent,\n",
    "            baseline_config_vector=c_vector,\n",
    "            start_ts=start_time,\n",
    "            end_ts=end_time,\n",
    "            obd_csv_path=obd_csv_path,\n",
    "            episodes=100,\n",
    "            trace=True\n",
    "        )\n",
    "\n",
    "        # === Save to results folder ===\n",
    "        results_dir = f\"./results/v0/test_{num}/\"\n",
    "        os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "        # Save reward plot\n",
    "        plt.figure()\n",
    "        plt.plot(rewards)\n",
    "        plt.title(\"Q-Learning Reward\", loc=\"left\")\n",
    "        plt.xlabel(\"Episode\")\n",
    "        plt.ylabel(\"Total Reward\")\n",
    "        plt.annotate(\n",
    "            f\"ε={init_epsilon:.2f}, γ={init_gamma:.2f}, α={init_alpha:.2f}\",\n",
    "            xy=(1.0, 1.01),\n",
    "            xycoords='axes fraction',\n",
    "            ha='right',\n",
    "            va='bottom',\n",
    "            fontsize=10\n",
    "        )\n",
    "        plt.grid(True)\n",
    "        plt.savefig(os.path.join(results_dir, \"reward_progress.png\"))\n",
    "        plt.close()\n",
    "\n",
    "        # Save Q-table\n",
    "        with open(os.path.join(results_dir, \"q_table.pkl\"), \"wb\") as f:\n",
    "            pickle.dump(agent.q_table, f)\n",
    "\n",
    "        # Save best config\n",
    "        best_config = agent.get_best_config_vector()\n",
    "        with open(os.path.join(results_dir, \"best_config.txt\"), \"w\") as f:\n",
    "            f.write(\",\".join(map(str, best_config)))\n",
    "\n",
    "        # Save test metadata\n",
    "        with open(os.path.join(results_dir, \"test_parameters.txt\"), \"w\") as f:\n",
    "            f.write(f\"csv_file_used: {obd_csv_path}\\n\")\n",
    "            f.write(f\"config_file: config_{num}.json\\n\")\n",
    "            f.write(f\"epsilon: {agent.epsilon}\\n\")\n",
    "            f.write(f\"gamma: {agent.gamma}\\n\")\n",
    "            f.write(f\"alpha: {agent.alpha}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f421905d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config vector 0 loaded: [12, 1, 13, 1, 39, 1, 300, 120, 1]\n",
      "Running training for config_0 with epsilon=0.3, gamma=0.5, alpha=0.7\n",
      "[Episode 0] Latency: 3510 ms\n",
      "[Episode 0] Action: 0, Reward: 2.100\n",
      "[Episode 1] Latency: 3510 ms\n",
      "[Episode 1] Action: 0, Reward: 1.300\n",
      "[Episode 2] Latency: 3510 ms\n",
      "[Episode 2] Action: 0, Reward: 0.600\n",
      "[Episode 3] Latency: 3510 ms\n",
      "[Episode 3] Action: 15, Reward: 0.600\n",
      "[Episode 4] Latency: 3510 ms\n",
      "[Episode 4] Action: 13, Reward: 0.600\n",
      "[Episode 5] Latency: 3510 ms\n",
      "[Episode 5] Action: 10, Reward: 0.600\n",
      "[Episode 6] Latency: 3510 ms\n",
      "[Episode 6] Action: 10, Reward: 0.600\n",
      "[Episode 7] Latency: 3510 ms\n",
      "[Episode 7] Action: 10, Reward: 0.600\n",
      "[Episode 8] Latency: 3510 ms\n",
      "[Episode 8] Action: 9, Reward: 0.600\n",
      "[Episode 9] Latency: 3510 ms\n",
      "[Episode 9] Action: 10, Reward: 0.600\n",
      "[Episode 10] Latency: 3510 ms\n",
      "[Episode 10] Action: 10, Reward: 0.600\n",
      "[Episode 11] Latency: 3510 ms\n",
      "[Episode 11] Action: 10, Reward: 0.600\n",
      "[Episode 12] Latency: 3510 ms\n",
      "[Episode 12] Action: 10, Reward: 0.600\n",
      "[Episode 13] Latency: 3510 ms\n",
      "[Episode 13] Action: 10, Reward: 0.600\n",
      "[Episode 14] Latency: 3510 ms\n",
      "[Episode 14] Action: 4, Reward: 0.600\n",
      "[Episode 15] Latency: 3510 ms\n",
      "[Episode 15] Action: 0, Reward: 0.900\n",
      "[Episode 16] Latency: 3510 ms\n",
      "[Episode 16] Action: 1, Reward: 2.100\n",
      "[Episode 17] Latency: 3510 ms\n",
      "[Episode 17] Action: 0, Reward: 3.300\n",
      "[Episode 18] Latency: 3510 ms\n",
      "[Episode 18] Action: 1, Reward: 2.500\n",
      "[Episode 19] Latency: 3510 ms\n",
      "[Episode 19] Action: 0, Reward: 1.700\n",
      "[Episode 20] Latency: 3510 ms\n",
      "[Episode 20] Action: 0, Reward: 1.000\n",
      "[Episode 21] Latency: 3510 ms\n",
      "[Episode 21] Action: 0, Reward: 1.300\n",
      "[Episode 22] Latency: 3510 ms\n",
      "[Episode 22] Action: 0, Reward: 2.500\n",
      "[Episode 23] Latency: 3510 ms\n",
      "[Episode 23] Action: 0, Reward: 1.700\n",
      "[Episode 24] Latency: 3510 ms\n",
      "[Episode 24] Action: 0, Reward: 1.000\n",
      "[Episode 25] Latency: 3510 ms\n",
      "[Episode 25] Action: 5, Reward: 1.000\n",
      "[Episode 26] Latency: 3510 ms\n",
      "[Episode 26] Action: 11, Reward: 1.000\n",
      "[Episode 27] Latency: 3510 ms\n",
      "[Episode 27] Action: 9, Reward: 1.000\n",
      "[Episode 28] Latency: 3510 ms\n",
      "[Episode 28] Action: 15, Reward: 1.000\n",
      "[Episode 29] Latency: 3510 ms\n",
      "[Episode 29] Action: 14, Reward: 1.000\n",
      "[Episode 30] Latency: 3510 ms\n",
      "[Episode 30] Action: 15, Reward: 1.000\n",
      "[Episode 31] Latency: 3510 ms\n",
      "[Episode 31] Action: 1, Reward: 0.300\n",
      "[Episode 32] Latency: 3510 ms\n",
      "[Episode 32] Action: 0, Reward: 0.600\n",
      "[Episode 33] Latency: 3510 ms\n",
      "[Episode 33] Action: 0, Reward: 1.800\n",
      "[Episode 34] Latency: 3510 ms\n",
      "[Episode 34] Action: 0, Reward: 1.000\n",
      "[Episode 35] Latency: 3510 ms\n",
      "[Episode 35] Action: 0, Reward: 0.300\n",
      "[Episode 36] Latency: 3510 ms\n",
      "[Episode 36] Action: 14, Reward: 0.300\n",
      "[Episode 37] Latency: 3510 ms\n",
      "[Episode 37] Action: 0, Reward: 0.600\n",
      "[Episode 38] Latency: 3510 ms\n",
      "[Episode 38] Action: 0, Reward: 1.800\n",
      "[Episode 39] Latency: 3510 ms\n",
      "[Episode 39] Action: 2, Reward: 3.000\n",
      "[Episode 40] Latency: 3510 ms\n",
      "[Episode 40] Action: 0, Reward: 2.200\n",
      "[Episode 41] Latency: 3510 ms\n",
      "[Episode 41] Action: 1, Reward: 2.500\n",
      "[Episode 42] Latency: 3510 ms\n",
      "[Episode 42] Action: 0, Reward: 1.800\n",
      "[Episode 43] Latency: 3510 ms\n",
      "[Episode 43] Action: 0, Reward: 2.100\n",
      "[Episode 44] Latency: 3510 ms\n",
      "[Episode 44] Action: 0, Reward: 3.300\n",
      "[Episode 45] Latency: 3510 ms\n",
      "[Episode 45] Action: 0, Reward: 2.500\n",
      "[Episode 46] Latency: 3510 ms\n",
      "[Episode 46] Action: 0, Reward: 1.800\n",
      "[Episode 47] Latency: 3510 ms\n",
      "[Episode 47] Action: 0, Reward: 2.100\n",
      "[Episode 48] Latency: 3510 ms\n",
      "[Episode 48] Action: 0, Reward: 3.300\n",
      "[Episode 49] Latency: 3510 ms\n",
      "[Episode 49] Action: 0, Reward: 2.500\n",
      "[Episode 50] Latency: 3510 ms\n",
      "[Episode 50] Action: 0, Reward: 1.800\n",
      "[Episode 51] Latency: 3510 ms\n",
      "[Episode 51] Action: 0, Reward: 2.100\n",
      "[Episode 52] Latency: 3510 ms\n",
      "[Episode 52] Action: 0, Reward: 3.300\n",
      "[Episode 53] Latency: 3510 ms\n",
      "[Episode 53] Action: 0, Reward: 2.500\n",
      "[Episode 54] Latency: 3510 ms\n",
      "[Episode 54] Action: 0, Reward: 1.800\n",
      "[Episode 55] Latency: 3510 ms\n",
      "[Episode 55] Action: 0, Reward: 2.100\n",
      "[Episode 56] Latency: 3510 ms\n",
      "[Episode 56] Action: 0, Reward: 3.300\n",
      "[Episode 57] Latency: 3510 ms\n",
      "[Episode 57] Action: 0, Reward: 2.500\n",
      "[Episode 58] Latency: 3510 ms\n",
      "[Episode 58] Action: 0, Reward: 1.800\n",
      "[Episode 59] Latency: 3510 ms\n",
      "[Episode 59] Action: 0, Reward: 2.100\n",
      "[Episode 60] Latency: 3510 ms\n",
      "[Episode 60] Action: 0, Reward: 3.300\n",
      "[Episode 61] Latency: 3510 ms\n",
      "[Episode 61] Action: 0, Reward: 2.500\n",
      "[Episode 62] Latency: 3510 ms\n",
      "[Episode 62] Action: 0, Reward: 1.800\n",
      "[Episode 63] Latency: 3510 ms\n",
      "[Episode 63] Action: 14, Reward: 1.800\n",
      "[Episode 64] Latency: 3510 ms\n",
      "[Episode 64] Action: 0, Reward: 2.100\n",
      "[Episode 65] Latency: 3510 ms\n",
      "[Episode 65] Action: 0, Reward: 3.300\n",
      "[Episode 66] Latency: 3510 ms\n",
      "[Episode 66] Action: 0, Reward: 2.500\n",
      "[Episode 67] Latency: 3510 ms\n",
      "[Episode 67] Action: 19, Reward: 2.500\n",
      "[Episode 68] Latency: 3510 ms\n",
      "[Episode 68] Action: 0, Reward: 1.800\n",
      "[Episode 69] Latency: 3510 ms\n",
      "[Episode 69] Action: 15, Reward: 1.800\n",
      "[Episode 70] Latency: 3510 ms\n",
      "[Episode 70] Action: 0, Reward: 2.100\n",
      "[Episode 71] Latency: 3510 ms\n",
      "[Episode 71] Action: 0, Reward: 3.300\n",
      "[Episode 72] Latency: 3510 ms\n",
      "[Episode 72] Action: 0, Reward: 2.500\n",
      "[Episode 73] Latency: 3510 ms\n",
      "[Episode 73] Action: 0, Reward: 1.800\n",
      "[Episode 74] Latency: 3510 ms\n",
      "[Episode 74] Action: 0, Reward: 2.100\n",
      "[Episode 75] Latency: 3510 ms\n",
      "[Episode 75] Action: 0, Reward: 3.300\n",
      "[Episode 76] Latency: 3510 ms\n",
      "[Episode 76] Action: 0, Reward: 2.500\n",
      "[Episode 77] Latency: 3510 ms\n",
      "[Episode 77] Action: 0, Reward: 1.800\n",
      "[Episode 78] Latency: 3510 ms\n",
      "[Episode 78] Action: 0, Reward: 2.100\n",
      "[Episode 79] Latency: 3510 ms\n",
      "[Episode 79] Action: 0, Reward: 3.300\n",
      "[Episode 80] Latency: 3510 ms\n",
      "[Episode 80] Action: 0, Reward: 2.500\n",
      "[Episode 81] Latency: 3510 ms\n",
      "[Episode 81] Action: 0, Reward: 1.800\n",
      "[Episode 82] Latency: 3510 ms\n",
      "[Episode 82] Action: 0, Reward: 2.100\n",
      "[Episode 83] Latency: 3510 ms\n",
      "[Episode 83] Action: 0, Reward: 3.300\n",
      "[Episode 84] Latency: 3510 ms\n",
      "[Episode 84] Action: 0, Reward: 2.500\n",
      "[Episode 85] Latency: 3510 ms\n",
      "[Episode 85] Action: 0, Reward: 1.800\n",
      "[Episode 86] Latency: 3510 ms\n",
      "[Episode 86] Action: 0, Reward: 2.100\n",
      "[Episode 87] Latency: 3510 ms\n",
      "[Episode 87] Action: 0, Reward: 3.300\n",
      "[Episode 88] Latency: 3510 ms\n",
      "[Episode 88] Action: 0, Reward: 2.500\n",
      "[Episode 89] Latency: 3510 ms\n",
      "[Episode 89] Action: 0, Reward: 1.800\n",
      "[Episode 90] Latency: 3510 ms\n",
      "[Episode 90] Action: 0, Reward: 2.100\n",
      "[Episode 91] Latency: 3510 ms\n",
      "[Episode 91] Action: 0, Reward: 3.300\n",
      "[Episode 92] Latency: 3510 ms\n",
      "[Episode 92] Action: 0, Reward: 2.500\n",
      "[Episode 93] Latency: 3510 ms\n",
      "[Episode 93] Action: 0, Reward: 1.800\n",
      "[Episode 94] Latency: 3510 ms\n",
      "[Episode 94] Action: 0, Reward: 2.100\n",
      "[Episode 95] Latency: 3510 ms\n",
      "[Episode 95] Action: 0, Reward: 3.300\n",
      "[Episode 96] Latency: 3510 ms\n",
      "[Episode 96] Action: 0, Reward: 2.500\n",
      "[Episode 97] Latency: 3510 ms\n",
      "[Episode 97] Action: 0, Reward: 1.800\n",
      "[Episode 98] Latency: 3510 ms\n",
      "[Episode 98] Action: 0, Reward: 2.100\n",
      "[Episode 99] Latency: 3510 ms\n",
      "[Episode 99] Action: 11, Reward: 2.100\n",
      "Config vector 1 loaded: [12, 2, 13, 2, 39, 2, 1, 1, 1]\n",
      "Running training for config_1 with epsilon=0.3, gamma=0.5, alpha=0.7\n",
      "[Episode 0] Latency: 401989 ms\n",
      "[Episode 0] Action: 0, Reward: 1.300\n",
      "[Episode 1] Latency: 401989 ms\n",
      "[Episode 1] Action: 0, Reward: 1.500\n",
      "[Episode 2] Latency: 401989 ms\n",
      "[Episode 2] Action: 0, Reward: 1.100\n",
      "[Episode 3] Latency: 401989 ms\n",
      "[Episode 3] Action: 9, Reward: 1.100\n",
      "[Episode 4] Latency: 401989 ms\n",
      "[Episode 4] Action: 9, Reward: 1.100\n",
      "[Episode 5] Latency: 401989 ms\n",
      "[Episode 5] Action: 5, Reward: 1.100\n",
      "[Episode 6] Latency: 401989 ms\n",
      "[Episode 6] Action: 0, Reward: 1.000\n",
      "[Episode 7] Latency: 401989 ms\n",
      "[Episode 7] Action: 0, Reward: 1.300\n",
      "[Episode 8] Latency: 401989 ms\n",
      "[Episode 8] Action: 11, Reward: 1.300\n",
      "[Episode 9] Latency: 401989 ms\n",
      "[Episode 9] Action: 0, Reward: 1.500\n",
      "[Episode 10] Latency: 401989 ms\n",
      "[Episode 10] Action: 0, Reward: 1.100\n",
      "[Episode 11] Latency: 401989 ms\n",
      "[Episode 11] Action: 1, Reward: 1.400\n",
      "[Episode 12] Latency: 401989 ms\n",
      "[Episode 12] Action: 0, Reward: 1.300\n",
      "[Episode 13] Latency: 401989 ms\n",
      "[Episode 13] Action: 0, Reward: 1.600\n",
      "[Episode 14] Latency: 401989 ms\n",
      "[Episode 14] Action: 0, Reward: 1.800\n",
      "[Episode 15] Latency: 401989 ms\n",
      "[Episode 15] Action: 18, Reward: 1.800\n",
      "[Episode 16] Latency: 401989 ms\n",
      "[Episode 16] Action: 0, Reward: 1.400\n",
      "[Episode 17] Latency: 401989 ms\n",
      "[Episode 17] Action: 0, Reward: 1.300\n",
      "[Episode 18] Latency: 401989 ms\n",
      "[Episode 18] Action: 0, Reward: 1.600\n",
      "[Episode 19] Latency: 401989 ms\n",
      "[Episode 19] Action: 0, Reward: 1.800\n",
      "[Episode 20] Latency: 401989 ms\n",
      "[Episode 20] Action: 0, Reward: 1.400\n",
      "[Episode 21] Latency: 401989 ms\n",
      "[Episode 21] Action: 0, Reward: 1.300\n",
      "[Episode 22] Latency: 401989 ms\n",
      "[Episode 22] Action: 0, Reward: 1.600\n",
      "[Episode 23] Latency: 401989 ms\n",
      "[Episode 23] Action: 0, Reward: 1.800\n",
      "[Episode 24] Latency: 401989 ms\n",
      "[Episode 24] Action: 10, Reward: 1.800\n",
      "[Episode 25] Latency: 401989 ms\n",
      "[Episode 25] Action: 0, Reward: 1.400\n",
      "[Episode 26] Latency: 401989 ms\n",
      "[Episode 26] Action: 0, Reward: 1.300\n",
      "[Episode 27] Latency: 401989 ms\n",
      "[Episode 27] Action: 0, Reward: 1.600\n",
      "[Episode 28] Latency: 401989 ms\n",
      "[Episode 28] Action: 0, Reward: 1.800\n",
      "[Episode 29] Latency: 401989 ms\n",
      "[Episode 29] Action: 0, Reward: 1.400\n",
      "[Episode 30] Latency: 401989 ms\n",
      "[Episode 30] Action: 0, Reward: 1.300\n",
      "[Episode 31] Latency: 401989 ms\n",
      "[Episode 31] Action: 0, Reward: 1.600\n",
      "[Episode 32] Latency: 401989 ms\n",
      "[Episode 32] Action: 7, Reward: 1.600\n",
      "[Episode 33] Latency: 401989 ms\n",
      "[Episode 33] Action: 0, Reward: 1.800\n",
      "[Episode 34] Latency: 401989 ms\n",
      "[Episode 34] Action: 0, Reward: 1.400\n",
      "[Episode 35] Latency: 401989 ms\n",
      "[Episode 35] Action: 8, Reward: 1.400\n",
      "[Episode 36] Latency: 401989 ms\n",
      "[Episode 36] Action: 0, Reward: 1.300\n",
      "[Episode 37] Latency: 401989 ms\n",
      "[Episode 37] Action: 0, Reward: 1.600\n",
      "[Episode 38] Latency: 401989 ms\n",
      "[Episode 38] Action: 0, Reward: 1.800\n",
      "[Episode 39] Latency: 401989 ms\n",
      "[Episode 39] Action: 0, Reward: 1.400\n",
      "[Episode 40] Latency: 401989 ms\n",
      "[Episode 40] Action: 3, Reward: 1.400\n",
      "[Episode 41] Latency: 401989 ms\n",
      "[Episode 41] Action: 0, Reward: 1.300\n",
      "[Episode 42] Latency: 401989 ms\n",
      "[Episode 42] Action: 0, Reward: 1.600\n",
      "[Episode 43] Latency: 401989 ms\n",
      "[Episode 43] Action: 0, Reward: 1.800\n",
      "[Episode 44] Latency: 401989 ms\n",
      "[Episode 44] Action: 0, Reward: 1.400\n",
      "[Episode 45] Latency: 401989 ms\n",
      "[Episode 45] Action: 0, Reward: 1.300\n",
      "[Episode 46] Latency: 401989 ms\n",
      "[Episode 46] Action: 0, Reward: 1.600\n",
      "[Episode 47] Latency: 401989 ms\n",
      "[Episode 47] Action: 5, Reward: 1.600\n",
      "[Episode 48] Latency: 401989 ms\n",
      "[Episode 48] Action: 0, Reward: 1.800\n",
      "[Episode 49] Latency: 401989 ms\n",
      "[Episode 49] Action: 13, Reward: 1.800\n",
      "[Episode 50] Latency: 401989 ms\n",
      "[Episode 50] Action: 0, Reward: 1.400\n",
      "[Episode 51] Latency: 401989 ms\n",
      "[Episode 51] Action: 0, Reward: 1.300\n",
      "[Episode 52] Latency: 401989 ms\n",
      "[Episode 52] Action: 0, Reward: 1.600\n",
      "[Episode 53] Latency: 401989 ms\n",
      "[Episode 53] Action: 0, Reward: 1.800\n",
      "[Episode 54] Latency: 401989 ms\n",
      "[Episode 54] Action: 12, Reward: 1.800\n",
      "[Episode 55] Latency: 401989 ms\n",
      "[Episode 55] Action: 0, Reward: 1.400\n",
      "[Episode 56] Latency: 401989 ms\n",
      "[Episode 56] Action: 4, Reward: 1.400\n",
      "[Episode 57] Latency: 401989 ms\n",
      "[Episode 57] Action: 0, Reward: 1.300\n",
      "[Episode 58] Latency: 401989 ms\n",
      "[Episode 58] Action: 0, Reward: 1.600\n",
      "[Episode 59] Latency: 401989 ms\n",
      "[Episode 59] Action: 0, Reward: 1.800\n",
      "[Episode 60] Latency: 401989 ms\n",
      "[Episode 60] Action: 0, Reward: 1.400\n",
      "[Episode 61] Latency: 401989 ms\n",
      "[Episode 61] Action: 0, Reward: 1.300\n",
      "[Episode 62] Latency: 401989 ms\n",
      "[Episode 62] Action: 0, Reward: 1.600\n",
      "[Episode 63] Latency: 401989 ms\n",
      "[Episode 63] Action: 0, Reward: 1.800\n",
      "[Episode 64] Latency: 401989 ms\n",
      "[Episode 64] Action: 17, Reward: 1.800\n",
      "[Episode 65] Latency: 401989 ms\n",
      "[Episode 65] Action: 0, Reward: 1.400\n",
      "[Episode 66] Latency: 401989 ms\n",
      "[Episode 66] Action: 13, Reward: 1.400\n",
      "[Episode 67] Latency: 401989 ms\n",
      "[Episode 67] Action: 0, Reward: 1.300\n",
      "[Episode 68] Latency: 401989 ms\n",
      "[Episode 68] Action: 0, Reward: 1.600\n",
      "[Episode 69] Latency: 401989 ms\n",
      "[Episode 69] Action: 0, Reward: 1.800\n",
      "[Episode 70] Latency: 401989 ms\n",
      "[Episode 70] Action: 0, Reward: 1.400\n",
      "[Episode 71] Latency: 401989 ms\n",
      "[Episode 71] Action: 0, Reward: 1.300\n",
      "[Episode 72] Latency: 401989 ms\n",
      "[Episode 72] Action: 0, Reward: 1.600\n",
      "[Episode 73] Latency: 401989 ms\n",
      "[Episode 73] Action: 0, Reward: 1.800\n",
      "[Episode 74] Latency: 401989 ms\n",
      "[Episode 74] Action: 0, Reward: 1.400\n",
      "[Episode 75] Latency: 401989 ms\n",
      "[Episode 75] Action: 0, Reward: 1.300\n",
      "[Episode 76] Latency: 401989 ms\n",
      "[Episode 76] Action: 0, Reward: 1.600\n",
      "[Episode 77] Latency: 401989 ms\n",
      "[Episode 77] Action: 0, Reward: 1.800\n",
      "[Episode 78] Latency: 401989 ms\n",
      "[Episode 78] Action: 0, Reward: 1.400\n",
      "[Episode 79] Latency: 401989 ms\n",
      "[Episode 79] Action: 0, Reward: 1.300\n",
      "[Episode 80] Latency: 401989 ms\n",
      "[Episode 80] Action: 0, Reward: 1.600\n",
      "[Episode 81] Latency: 401989 ms\n",
      "[Episode 81] Action: 0, Reward: 1.800\n",
      "[Episode 82] Latency: 401989 ms\n",
      "[Episode 82] Action: 0, Reward: 1.400\n",
      "[Episode 83] Latency: 401989 ms\n",
      "[Episode 83] Action: 0, Reward: 1.300\n",
      "[Episode 84] Latency: 401989 ms\n",
      "[Episode 84] Action: 0, Reward: 1.600\n",
      "[Episode 85] Latency: 401989 ms\n",
      "[Episode 85] Action: 0, Reward: 1.800\n",
      "[Episode 86] Latency: 401989 ms\n",
      "[Episode 86] Action: 0, Reward: 1.400\n",
      "[Episode 87] Latency: 401989 ms\n",
      "[Episode 87] Action: 0, Reward: 1.300\n",
      "[Episode 88] Latency: 401989 ms\n",
      "[Episode 88] Action: 0, Reward: 1.600\n",
      "[Episode 89] Latency: 401989 ms\n",
      "[Episode 89] Action: 0, Reward: 1.800\n",
      "[Episode 90] Latency: 401989 ms\n",
      "[Episode 90] Action: 1, Reward: 2.000\n",
      "[Episode 91] Latency: 401989 ms\n",
      "[Episode 91] Action: 0, Reward: 1.600\n",
      "[Episode 92] Latency: 401989 ms\n",
      "[Episode 92] Action: 0, Reward: 1.500\n",
      "[Episode 93] Latency: 401989 ms\n",
      "[Episode 93] Action: 0, Reward: 1.800\n",
      "[Episode 94] Latency: 401989 ms\n",
      "[Episode 94] Action: 0, Reward: 2.000\n",
      "[Episode 95] Latency: 401989 ms\n",
      "[Episode 95] Action: 0, Reward: 1.600\n",
      "[Episode 96] Latency: 401989 ms\n",
      "[Episode 96] Action: 0, Reward: 1.500\n",
      "[Episode 97] Latency: 401989 ms\n",
      "[Episode 97] Action: 0, Reward: 1.800\n",
      "[Episode 98] Latency: 401989 ms\n",
      "[Episode 98] Action: 0, Reward: 2.000\n",
      "[Episode 99] Latency: 401989 ms\n",
      "[Episode 99] Action: 0, Reward: 1.600\n",
      "Config vector 2 loaded: [12, 2, 13, 3, 39, 3, 30, 60, 1]\n",
      "Running training for config_2 with epsilon=0.3, gamma=0.5, alpha=0.7\n",
      "[Episode 0] Latency: 3670 ms\n",
      "[Episode 0] Action: 0, Reward: 3.300\n",
      "[Episode 1] Latency: 3670 ms\n",
      "[Episode 1] Action: 0, Reward: 3.500\n",
      "[Episode 2] Latency: 3670 ms\n",
      "[Episode 2] Action: 4, Reward: 3.500\n",
      "[Episode 3] Latency: 3670 ms\n",
      "[Episode 3] Action: 0, Reward: 3.100\n",
      "[Episode 4] Latency: 3670 ms\n",
      "[Episode 4] Action: 0, Reward: 3.000\n",
      "[Episode 5] Latency: 3670 ms\n",
      "[Episode 5] Action: 0, Reward: 3.300\n",
      "[Episode 6] Latency: 3670 ms\n",
      "[Episode 6] Action: 17, Reward: 3.300\n",
      "[Episode 7] Latency: 3670 ms\n",
      "[Episode 7] Action: 0, Reward: 3.500\n",
      "[Episode 8] Latency: 3670 ms\n",
      "[Episode 8] Action: 0, Reward: 3.100\n",
      "[Episode 9] Latency: 3670 ms\n",
      "[Episode 9] Action: 0, Reward: 3.000\n",
      "[Episode 10] Latency: 3670 ms\n",
      "[Episode 10] Action: 0, Reward: 3.300\n",
      "[Episode 11] Latency: 3670 ms\n",
      "[Episode 11] Action: 0, Reward: 3.500\n",
      "[Episode 12] Latency: 3670 ms\n",
      "[Episode 12] Action: 0, Reward: 3.100\n",
      "[Episode 13] Latency: 3670 ms\n",
      "[Episode 13] Action: 0, Reward: 3.000\n",
      "[Episode 14] Latency: 3670 ms\n",
      "[Episode 14] Action: 0, Reward: 3.300\n",
      "[Episode 15] Latency: 3670 ms\n",
      "[Episode 15] Action: 0, Reward: 3.500\n",
      "[Episode 16] Latency: 3670 ms\n",
      "[Episode 16] Action: 5, Reward: 3.500\n",
      "[Episode 17] Latency: 3670 ms\n",
      "[Episode 17] Action: 0, Reward: 3.100\n",
      "[Episode 18] Latency: 3670 ms\n",
      "[Episode 18] Action: 0, Reward: 3.000\n",
      "[Episode 19] Latency: 3670 ms\n",
      "[Episode 19] Action: 0, Reward: 3.300\n",
      "[Episode 20] Latency: 3670 ms\n",
      "[Episode 20] Action: 0, Reward: 3.500\n",
      "[Episode 21] Latency: 3670 ms\n",
      "[Episode 21] Action: 0, Reward: 3.100\n",
      "[Episode 22] Latency: 3670 ms\n",
      "[Episode 22] Action: 0, Reward: 3.000\n",
      "[Episode 23] Latency: 3670 ms\n",
      "[Episode 23] Action: 0, Reward: 3.300\n",
      "[Episode 24] Latency: 3670 ms\n",
      "[Episode 24] Action: 0, Reward: 3.500\n",
      "[Episode 25] Latency: 3670 ms\n",
      "[Episode 25] Action: 0, Reward: 3.100\n",
      "[Episode 26] Latency: 3670 ms\n",
      "[Episode 26] Action: 0, Reward: 3.000\n",
      "[Episode 27] Latency: 3670 ms\n",
      "[Episode 27] Action: 0, Reward: 3.300\n",
      "[Episode 28] Latency: 3670 ms\n",
      "[Episode 28] Action: 0, Reward: 3.500\n",
      "[Episode 29] Latency: 3670 ms\n",
      "[Episode 29] Action: 0, Reward: 3.100\n",
      "[Episode 30] Latency: 3670 ms\n",
      "[Episode 30] Action: 0, Reward: 3.000\n",
      "[Episode 31] Latency: 3670 ms\n",
      "[Episode 31] Action: 0, Reward: 3.300\n",
      "[Episode 32] Latency: 3670 ms\n",
      "[Episode 32] Action: 0, Reward: 3.500\n",
      "[Episode 33] Latency: 3670 ms\n",
      "[Episode 33] Action: 0, Reward: 3.100\n",
      "[Episode 34] Latency: 3670 ms\n",
      "[Episode 34] Action: 17, Reward: 3.100\n",
      "[Episode 35] Latency: 3670 ms\n",
      "[Episode 35] Action: 0, Reward: 3.000\n",
      "[Episode 36] Latency: 3670 ms\n",
      "[Episode 36] Action: 0, Reward: 3.300\n",
      "[Episode 37] Latency: 3670 ms\n",
      "[Episode 37] Action: 0, Reward: 3.500\n",
      "[Episode 38] Latency: 3670 ms\n",
      "[Episode 38] Action: 0, Reward: 3.100\n",
      "[Episode 39] Latency: 3670 ms\n",
      "[Episode 39] Action: 0, Reward: 3.000\n",
      "[Episode 40] Latency: 3670 ms\n",
      "[Episode 40] Action: 0, Reward: 3.300\n",
      "[Episode 41] Latency: 3670 ms\n",
      "[Episode 41] Action: 0, Reward: 3.500\n",
      "[Episode 42] Latency: 3670 ms\n",
      "[Episode 42] Action: 0, Reward: 3.100\n",
      "[Episode 43] Latency: 3670 ms\n",
      "[Episode 43] Action: 0, Reward: 3.000\n",
      "[Episode 44] Latency: 3670 ms\n",
      "[Episode 44] Action: 0, Reward: 3.300\n",
      "[Episode 45] Latency: 3670 ms\n",
      "[Episode 45] Action: 0, Reward: 3.500\n",
      "[Episode 46] Latency: 3670 ms\n",
      "[Episode 46] Action: 0, Reward: 3.100\n",
      "[Episode 47] Latency: 3670 ms\n",
      "[Episode 47] Action: 5, Reward: 3.100\n",
      "[Episode 48] Latency: 3670 ms\n",
      "[Episode 48] Action: 0, Reward: 3.000\n",
      "[Episode 49] Latency: 3670 ms\n",
      "[Episode 49] Action: 19, Reward: 3.000\n",
      "[Episode 50] Latency: 3670 ms\n",
      "[Episode 50] Action: 0, Reward: 3.300\n",
      "[Episode 51] Latency: 3670 ms\n",
      "[Episode 51] Action: 0, Reward: 3.500\n",
      "[Episode 52] Latency: 3670 ms\n",
      "[Episode 52] Action: 0, Reward: 3.100\n",
      "[Episode 53] Latency: 3670 ms\n",
      "[Episode 53] Action: 0, Reward: 3.000\n",
      "[Episode 54] Latency: 3670 ms\n",
      "[Episode 54] Action: 0, Reward: 3.300\n",
      "[Episode 55] Latency: 3670 ms\n",
      "[Episode 55] Action: 0, Reward: 3.500\n",
      "[Episode 56] Latency: 3670 ms\n",
      "[Episode 56] Action: 0, Reward: 3.100\n",
      "[Episode 57] Latency: 3670 ms\n",
      "[Episode 57] Action: 7, Reward: 3.100\n",
      "[Episode 58] Latency: 3670 ms\n",
      "[Episode 58] Action: 11, Reward: 3.100\n",
      "[Episode 59] Latency: 3670 ms\n",
      "[Episode 59] Action: 0, Reward: 3.000\n",
      "[Episode 60] Latency: 3670 ms\n",
      "[Episode 60] Action: 0, Reward: 3.300\n",
      "[Episode 61] Latency: 3670 ms\n",
      "[Episode 61] Action: 0, Reward: 3.500\n",
      "[Episode 62] Latency: 3670 ms\n",
      "[Episode 62] Action: 0, Reward: 3.100\n",
      "[Episode 63] Latency: 3670 ms\n",
      "[Episode 63] Action: 4, Reward: 3.100\n",
      "[Episode 64] Latency: 3670 ms\n",
      "[Episode 64] Action: 0, Reward: 3.000\n",
      "[Episode 65] Latency: 3670 ms\n",
      "[Episode 65] Action: 0, Reward: 3.300\n",
      "[Episode 66] Latency: 3670 ms\n",
      "[Episode 66] Action: 0, Reward: 3.500\n",
      "[Episode 67] Latency: 3670 ms\n",
      "[Episode 67] Action: 0, Reward: 3.100\n",
      "[Episode 68] Latency: 3670 ms\n",
      "[Episode 68] Action: 0, Reward: 3.000\n",
      "[Episode 69] Latency: 3670 ms\n",
      "[Episode 69] Action: 0, Reward: 3.300\n",
      "[Episode 70] Latency: 3670 ms\n",
      "[Episode 70] Action: 0, Reward: 3.500\n",
      "[Episode 71] Latency: 3670 ms\n",
      "[Episode 71] Action: 0, Reward: 3.100\n",
      "[Episode 72] Latency: 3670 ms\n",
      "[Episode 72] Action: 0, Reward: 3.000\n",
      "[Episode 73] Latency: 3670 ms\n",
      "[Episode 73] Action: 0, Reward: 3.300\n",
      "[Episode 74] Latency: 3670 ms\n",
      "[Episode 74] Action: 0, Reward: 3.500\n",
      "[Episode 75] Latency: 3670 ms\n",
      "[Episode 75] Action: 0, Reward: 3.100\n",
      "[Episode 76] Latency: 3670 ms\n",
      "[Episode 76] Action: 0, Reward: 3.000\n",
      "[Episode 77] Latency: 3670 ms\n",
      "[Episode 77] Action: 0, Reward: 3.300\n",
      "[Episode 78] Latency: 3670 ms\n",
      "[Episode 78] Action: 0, Reward: 3.500\n",
      "[Episode 79] Latency: 3670 ms\n",
      "[Episode 79] Action: 0, Reward: 3.100\n",
      "[Episode 80] Latency: 3670 ms\n",
      "[Episode 80] Action: 0, Reward: 3.000\n",
      "[Episode 81] Latency: 3670 ms\n",
      "[Episode 81] Action: 0, Reward: 3.300\n",
      "[Episode 82] Latency: 3670 ms\n",
      "[Episode 82] Action: 0, Reward: 3.500\n",
      "[Episode 83] Latency: 3670 ms\n",
      "[Episode 83] Action: 0, Reward: 3.100\n",
      "[Episode 84] Latency: 3670 ms\n",
      "[Episode 84] Action: 0, Reward: 3.000\n",
      "[Episode 85] Latency: 3670 ms\n",
      "[Episode 85] Action: 0, Reward: 3.300\n",
      "[Episode 86] Latency: 3670 ms\n",
      "[Episode 86] Action: 0, Reward: 3.500\n",
      "[Episode 87] Latency: 3670 ms\n",
      "[Episode 87] Action: 0, Reward: 3.100\n",
      "[Episode 88] Latency: 3670 ms\n",
      "[Episode 88] Action: 0, Reward: 3.000\n",
      "[Episode 89] Latency: 3670 ms\n",
      "[Episode 89] Action: 0, Reward: 3.300\n",
      "[Episode 90] Latency: 3670 ms\n",
      "[Episode 90] Action: 0, Reward: 3.500\n",
      "[Episode 91] Latency: 3670 ms\n",
      "[Episode 91] Action: 0, Reward: 3.100\n",
      "[Episode 92] Latency: 3670 ms\n",
      "[Episode 92] Action: 0, Reward: 3.000\n",
      "[Episode 93] Latency: 3670 ms\n",
      "[Episode 93] Action: 0, Reward: 3.300\n",
      "[Episode 94] Latency: 3670 ms\n",
      "[Episode 94] Action: 0, Reward: 3.500\n",
      "[Episode 95] Latency: 3670 ms\n",
      "[Episode 95] Action: 0, Reward: 3.100\n",
      "[Episode 96] Latency: 3670 ms\n",
      "[Episode 96] Action: 0, Reward: 3.000\n",
      "[Episode 97] Latency: 3670 ms\n",
      "[Episode 97] Action: 0, Reward: 3.300\n",
      "[Episode 98] Latency: 3670 ms\n",
      "[Episode 98] Action: 0, Reward: 3.500\n",
      "[Episode 99] Latency: 3670 ms\n",
      "[Episode 99] Action: 0, Reward: 3.100\n",
      "Config vector 3 loaded: [12, 1, 13, 1, 39, 1, 300, 120, 2]\n",
      "Running training for config_3 with epsilon=0.3, gamma=0.5, alpha=0.7\n",
      "[Episode 0] Latency: 34663 ms\n",
      "[Episode 0] Action: 0, Reward: 1.200\n",
      "[Episode 1] Latency: 34663 ms\n",
      "[Episode 1] Action: 0, Reward: 1.500\n",
      "[Episode 2] Latency: 34663 ms\n",
      "[Episode 2] Action: 0, Reward: 1.700\n",
      "[Episode 3] Latency: 34663 ms\n",
      "[Episode 3] Action: 0, Reward: 1.300\n",
      "[Episode 4] Latency: 34663 ms\n",
      "[Episode 4] Action: 0, Reward: 1.200\n",
      "[Episode 5] Latency: 34663 ms\n",
      "[Episode 5] Action: 19, Reward: 1.200\n",
      "[Episode 6] Latency: 34663 ms\n",
      "[Episode 6] Action: 0, Reward: 1.500\n",
      "[Episode 7] Latency: 34663 ms\n",
      "[Episode 7] Action: 0, Reward: 1.700\n",
      "[Episode 8] Latency: 34663 ms\n",
      "[Episode 8] Action: 0, Reward: 1.300\n",
      "[Episode 9] Latency: 34663 ms\n",
      "[Episode 9] Action: 0, Reward: 1.200\n",
      "[Episode 10] Latency: 34663 ms\n",
      "[Episode 10] Action: 0, Reward: 1.500\n",
      "[Episode 11] Latency: 34663 ms\n",
      "[Episode 11] Action: 18, Reward: 1.500\n",
      "[Episode 12] Latency: 34663 ms\n",
      "[Episode 12] Action: 0, Reward: 1.700\n",
      "[Episode 13] Latency: 34663 ms\n",
      "[Episode 13] Action: 0, Reward: 1.300\n",
      "[Episode 14] Latency: 34663 ms\n",
      "[Episode 14] Action: 2, Reward: 1.200\n",
      "[Episode 15] Latency: 34663 ms\n",
      "[Episode 15] Action: 0, Reward: 1.100\n",
      "[Episode 16] Latency: 34663 ms\n",
      "[Episode 16] Action: 0, Reward: 1.400\n",
      "[Episode 17] Latency: 34663 ms\n",
      "[Episode 17] Action: 0, Reward: 1.600\n",
      "[Episode 18] Latency: 34663 ms\n",
      "[Episode 18] Action: 0, Reward: 1.200\n",
      "[Episode 19] Latency: 34663 ms\n",
      "[Episode 19] Action: 0, Reward: 1.100\n",
      "[Episode 20] Latency: 34663 ms\n",
      "[Episode 20] Action: 0, Reward: 1.400\n",
      "[Episode 21] Latency: 34663 ms\n",
      "[Episode 21] Action: 0, Reward: 1.600\n",
      "[Episode 22] Latency: 34663 ms\n",
      "[Episode 22] Action: 0, Reward: 1.200\n",
      "[Episode 23] Latency: 34663 ms\n",
      "[Episode 23] Action: 0, Reward: 1.100\n",
      "[Episode 24] Latency: 34663 ms\n",
      "[Episode 24] Action: 2, Reward: 1.400\n",
      "[Episode 25] Latency: 34663 ms\n",
      "[Episode 25] Action: 0, Reward: 1.700\n",
      "[Episode 26] Latency: 34663 ms\n",
      "[Episode 26] Action: 0, Reward: 1.900\n",
      "[Episode 27] Latency: 34663 ms\n",
      "[Episode 27] Action: 0, Reward: 1.500\n",
      "[Episode 28] Latency: 34663 ms\n",
      "[Episode 28] Action: 10, Reward: 1.500\n",
      "[Episode 29] Latency: 34663 ms\n",
      "[Episode 29] Action: 0, Reward: 1.400\n",
      "[Episode 30] Latency: 34663 ms\n",
      "[Episode 30] Action: 0, Reward: 1.700\n",
      "[Episode 31] Latency: 34663 ms\n",
      "[Episode 31] Action: 0, Reward: 1.900\n",
      "[Episode 32] Latency: 34663 ms\n",
      "[Episode 32] Action: 18, Reward: 1.900\n",
      "[Episode 33] Latency: 34663 ms\n",
      "[Episode 33] Action: 8, Reward: 1.900\n",
      "[Episode 34] Latency: 34663 ms\n",
      "[Episode 34] Action: 8, Reward: 1.900\n",
      "[Episode 35] Latency: 34663 ms\n",
      "[Episode 35] Action: 8, Reward: 1.900\n",
      "[Episode 36] Latency: 34663 ms\n",
      "[Episode 36] Action: 0, Reward: 1.500\n",
      "[Episode 37] Latency: 34663 ms\n",
      "[Episode 37] Action: 0, Reward: 1.400\n",
      "[Episode 38] Latency: 34663 ms\n",
      "[Episode 38] Action: 0, Reward: 1.700\n",
      "[Episode 39] Latency: 34663 ms\n",
      "[Episode 39] Action: 3, Reward: 1.700\n",
      "[Episode 40] Latency: 34663 ms\n",
      "[Episode 40] Action: 0, Reward: 1.900\n",
      "[Episode 41] Latency: 34663 ms\n",
      "[Episode 41] Action: 0, Reward: 1.500\n",
      "[Episode 42] Latency: 34663 ms\n",
      "[Episode 42] Action: 0, Reward: 1.400\n",
      "[Episode 43] Latency: 34663 ms\n",
      "[Episode 43] Action: 0, Reward: 1.700\n",
      "[Episode 44] Latency: 34663 ms\n",
      "[Episode 44] Action: 0, Reward: 1.900\n",
      "[Episode 45] Latency: 34663 ms\n",
      "[Episode 45] Action: 0, Reward: 1.500\n",
      "[Episode 46] Latency: 34663 ms\n",
      "[Episode 46] Action: 0, Reward: 1.400\n",
      "[Episode 47] Latency: 34663 ms\n",
      "[Episode 47] Action: 0, Reward: 1.700\n",
      "[Episode 48] Latency: 34663 ms\n",
      "[Episode 48] Action: 0, Reward: 1.900\n",
      "[Episode 49] Latency: 34663 ms\n",
      "[Episode 49] Action: 13, Reward: 1.900\n",
      "[Episode 50] Latency: 34663 ms\n",
      "[Episode 50] Action: 0, Reward: 1.500\n",
      "[Episode 51] Latency: 34663 ms\n",
      "[Episode 51] Action: 0, Reward: 1.400\n",
      "[Episode 52] Latency: 34663 ms\n",
      "[Episode 52] Action: 0, Reward: 1.700\n",
      "[Episode 53] Latency: 34663 ms\n",
      "[Episode 53] Action: 0, Reward: 1.900\n",
      "[Episode 54] Latency: 34663 ms\n",
      "[Episode 54] Action: 0, Reward: 1.500\n",
      "[Episode 55] Latency: 34663 ms\n",
      "[Episode 55] Action: 0, Reward: 1.400\n",
      "[Episode 56] Latency: 34663 ms\n",
      "[Episode 56] Action: 0, Reward: 1.700\n",
      "[Episode 57] Latency: 34663 ms\n",
      "[Episode 57] Action: 0, Reward: 1.900\n",
      "[Episode 58] Latency: 34663 ms\n",
      "[Episode 58] Action: 16, Reward: 1.900\n",
      "[Episode 59] Latency: 34663 ms\n",
      "[Episode 59] Action: 0, Reward: 1.500\n",
      "[Episode 60] Latency: 34663 ms\n",
      "[Episode 60] Action: 0, Reward: 1.400\n",
      "[Episode 61] Latency: 34663 ms\n",
      "[Episode 61] Action: 0, Reward: 1.700\n",
      "[Episode 62] Latency: 34663 ms\n",
      "[Episode 62] Action: 0, Reward: 1.900\n",
      "[Episode 63] Latency: 34663 ms\n",
      "[Episode 63] Action: 0, Reward: 1.500\n",
      "[Episode 64] Latency: 34663 ms\n",
      "[Episode 64] Action: 0, Reward: 1.400\n",
      "[Episode 65] Latency: 34663 ms\n",
      "[Episode 65] Action: 0, Reward: 1.700\n",
      "[Episode 66] Latency: 34663 ms\n",
      "[Episode 66] Action: 0, Reward: 1.900\n",
      "[Episode 67] Latency: 34663 ms\n",
      "[Episode 67] Action: 0, Reward: 1.500\n",
      "[Episode 68] Latency: 34663 ms\n",
      "[Episode 68] Action: 17, Reward: 1.500\n",
      "[Episode 69] Latency: 34663 ms\n",
      "[Episode 69] Action: 17, Reward: 1.500\n",
      "[Episode 70] Latency: 34663 ms\n",
      "[Episode 70] Action: 0, Reward: 1.400\n",
      "[Episode 71] Latency: 34663 ms\n",
      "[Episode 71] Action: 0, Reward: 1.700\n",
      "[Episode 72] Latency: 34663 ms\n",
      "[Episode 72] Action: 0, Reward: 1.900\n",
      "[Episode 73] Latency: 34663 ms\n",
      "[Episode 73] Action: 0, Reward: 1.500\n",
      "[Episode 74] Latency: 34663 ms\n",
      "[Episode 74] Action: 0, Reward: 1.400\n",
      "[Episode 75] Latency: 34663 ms\n",
      "[Episode 75] Action: 0, Reward: 1.700\n",
      "[Episode 76] Latency: 34663 ms\n",
      "[Episode 76] Action: 3, Reward: 1.700\n",
      "[Episode 77] Latency: 34663 ms\n",
      "[Episode 77] Action: 0, Reward: 1.900\n",
      "[Episode 78] Latency: 34663 ms\n",
      "[Episode 78] Action: 0, Reward: 1.500\n",
      "[Episode 79] Latency: 34663 ms\n",
      "[Episode 79] Action: 0, Reward: 1.400\n",
      "[Episode 80] Latency: 34663 ms\n",
      "[Episode 80] Action: 0, Reward: 1.700\n",
      "[Episode 81] Latency: 34663 ms\n",
      "[Episode 81] Action: 0, Reward: 1.900\n",
      "[Episode 82] Latency: 34663 ms\n",
      "[Episode 82] Action: 0, Reward: 1.500\n",
      "[Episode 83] Latency: 34663 ms\n",
      "[Episode 83] Action: 0, Reward: 1.400\n",
      "[Episode 84] Latency: 34663 ms\n",
      "[Episode 84] Action: 0, Reward: 1.700\n",
      "[Episode 85] Latency: 34663 ms\n",
      "[Episode 85] Action: 0, Reward: 1.900\n",
      "[Episode 86] Latency: 34663 ms\n",
      "[Episode 86] Action: 0, Reward: 1.500\n",
      "[Episode 87] Latency: 34663 ms\n",
      "[Episode 87] Action: 11, Reward: 1.500\n",
      "[Episode 88] Latency: 34663 ms\n",
      "[Episode 88] Action: 0, Reward: 1.400\n",
      "[Episode 89] Latency: 34663 ms\n",
      "[Episode 89] Action: 10, Reward: 1.400\n",
      "[Episode 90] Latency: 34663 ms\n",
      "[Episode 90] Action: 0, Reward: 1.700\n",
      "[Episode 91] Latency: 34663 ms\n",
      "[Episode 91] Action: 19, Reward: 1.700\n",
      "[Episode 92] Latency: 34663 ms\n",
      "[Episode 92] Action: 0, Reward: 1.900\n",
      "[Episode 93] Latency: 34663 ms\n",
      "[Episode 93] Action: 0, Reward: 1.500\n",
      "[Episode 94] Latency: 34663 ms\n",
      "[Episode 94] Action: 0, Reward: 1.400\n",
      "[Episode 95] Latency: 34663 ms\n",
      "[Episode 95] Action: 0, Reward: 1.700\n",
      "[Episode 96] Latency: 34663 ms\n",
      "[Episode 96] Action: 0, Reward: 1.900\n",
      "[Episode 97] Latency: 34663 ms\n",
      "[Episode 97] Action: 0, Reward: 1.500\n",
      "[Episode 98] Latency: 34663 ms\n",
      "[Episode 98] Action: 0, Reward: 1.400\n",
      "[Episode 99] Latency: 34663 ms\n",
      "[Episode 99] Action: 11, Reward: 1.400\n",
      "Config vector 4 loaded: [12, 1, 13, 1, 39, 1, 300, 120, 5]\n",
      "Running training for config_4 with epsilon=0.3, gamma=0.5, alpha=0.7\n",
      "[Episode 0] Latency: 763672 ms\n",
      "[Episode 0] Action: 0, Reward: 0.100\n",
      "[Episode 1] Latency: 763672 ms\n",
      "[Episode 1] Action: 0, Reward: -0.700\n",
      "[Episode 2] Latency: 763672 ms\n",
      "[Episode 2] Action: 0, Reward: -1.400\n",
      "[Episode 3] Latency: 763672 ms\n",
      "[Episode 3] Action: 0, Reward: -1.100\n",
      "[Episode 4] Latency: 763672 ms\n",
      "[Episode 4] Action: 0, Reward: 0.100\n",
      "[Episode 5] Latency: 763672 ms\n",
      "[Episode 5] Action: 1, Reward: 1.300\n",
      "[Episode 6] Latency: 763672 ms\n",
      "[Episode 6] Action: 0, Reward: 0.500\n",
      "[Episode 7] Latency: 763672 ms\n",
      "[Episode 7] Action: 1, Reward: -0.300\n",
      "[Episode 8] Latency: 763672 ms\n",
      "[Episode 8] Action: 0, Reward: -1.000\n",
      "[Episode 9] Latency: 763672 ms\n",
      "[Episode 9] Action: 5, Reward: -1.000\n",
      "[Episode 10] Latency: 763672 ms\n",
      "[Episode 10] Action: 17, Reward: -1.000\n",
      "[Episode 11] Latency: 763672 ms\n",
      "[Episode 11] Action: 0, Reward: -0.700\n",
      "[Episode 12] Latency: 763672 ms\n",
      "[Episode 12] Action: 0, Reward: 0.500\n",
      "[Episode 13] Latency: 763672 ms\n",
      "[Episode 13] Action: 0, Reward: -0.300\n",
      "[Episode 14] Latency: 763672 ms\n",
      "[Episode 14] Action: 0, Reward: -1.000\n",
      "[Episode 15] Latency: 763672 ms\n",
      "[Episode 15] Action: 5, Reward: -1.000\n",
      "[Episode 16] Latency: 763672 ms\n",
      "[Episode 16] Action: 17, Reward: -1.000\n",
      "[Episode 17] Latency: 763672 ms\n",
      "[Episode 17] Action: 0, Reward: -0.700\n",
      "[Episode 18] Latency: 763672 ms\n",
      "[Episode 18] Action: 0, Reward: 0.500\n",
      "[Episode 19] Latency: 763672 ms\n",
      "[Episode 19] Action: 0, Reward: -0.300\n",
      "[Episode 20] Latency: 763672 ms\n",
      "[Episode 20] Action: 0, Reward: -1.000\n",
      "[Episode 21] Latency: 763672 ms\n",
      "[Episode 21] Action: 3, Reward: -1.000\n",
      "[Episode 22] Latency: 763672 ms\n",
      "[Episode 22] Action: 0, Reward: -0.700\n",
      "[Episode 23] Latency: 763672 ms\n",
      "[Episode 23] Action: 0, Reward: 0.500\n",
      "[Episode 24] Latency: 763672 ms\n",
      "[Episode 24] Action: 0, Reward: -0.300\n",
      "[Episode 25] Latency: 763672 ms\n",
      "[Episode 25] Action: 0, Reward: -1.000\n",
      "[Episode 26] Latency: 763672 ms\n",
      "[Episode 26] Action: 13, Reward: -1.000\n",
      "[Episode 27] Latency: 763672 ms\n",
      "[Episode 27] Action: 0, Reward: -0.700\n",
      "[Episode 28] Latency: 763672 ms\n",
      "[Episode 28] Action: 0, Reward: 0.500\n",
      "[Episode 29] Latency: 763672 ms\n",
      "[Episode 29] Action: 0, Reward: -0.300\n",
      "[Episode 30] Latency: 763672 ms\n",
      "[Episode 30] Action: 0, Reward: -1.000\n",
      "[Episode 31] Latency: 763672 ms\n",
      "[Episode 31] Action: 1, Reward: -1.700\n",
      "[Episode 32] Latency: 763672 ms\n",
      "[Episode 32] Action: 15, Reward: -1.700\n",
      "[Episode 33] Latency: 763672 ms\n",
      "[Episode 33] Action: 0, Reward: -1.400\n",
      "[Episode 34] Latency: 763672 ms\n",
      "[Episode 34] Action: 0, Reward: -0.200\n",
      "[Episode 35] Latency: 763672 ms\n",
      "[Episode 35] Action: 0, Reward: -1.000\n",
      "[Episode 36] Latency: 763672 ms\n",
      "[Episode 36] Action: 0, Reward: -1.700\n",
      "[Episode 37] Latency: 763672 ms\n",
      "[Episode 37] Action: 1, Reward: -1.400\n",
      "[Episode 38] Latency: 763672 ms\n",
      "[Episode 38] Action: 10, Reward: -1.400\n",
      "[Episode 39] Latency: 763672 ms\n",
      "[Episode 39] Action: 0, Reward: -1.100\n",
      "[Episode 40] Latency: 763672 ms\n",
      "[Episode 40] Action: 0, Reward: 0.100\n",
      "[Episode 41] Latency: 763672 ms\n",
      "[Episode 41] Action: 0, Reward: -0.700\n",
      "[Episode 42] Latency: 763672 ms\n",
      "[Episode 42] Action: 0, Reward: -1.400\n",
      "[Episode 43] Latency: 763672 ms\n",
      "[Episode 43] Action: 1, Reward: -0.200\n",
      "[Episode 44] Latency: 763672 ms\n",
      "[Episode 44] Action: 0, Reward: 0.100\n",
      "[Episode 45] Latency: 763672 ms\n",
      "[Episode 45] Action: 0, Reward: 1.300\n",
      "[Episode 46] Latency: 763672 ms\n",
      "[Episode 46] Action: 0, Reward: 0.500\n",
      "[Episode 47] Latency: 763672 ms\n",
      "[Episode 47] Action: 0, Reward: -0.200\n",
      "[Episode 48] Latency: 763672 ms\n",
      "[Episode 48] Action: 0, Reward: 0.100\n",
      "[Episode 49] Latency: 763672 ms\n",
      "[Episode 49] Action: 0, Reward: 1.300\n",
      "[Episode 50] Latency: 763672 ms\n",
      "[Episode 50] Action: 3, Reward: 1.300\n",
      "[Episode 51] Latency: 763672 ms\n",
      "[Episode 51] Action: 0, Reward: 0.500\n",
      "[Episode 52] Latency: 763672 ms\n",
      "[Episode 52] Action: 0, Reward: -0.200\n",
      "[Episode 53] Latency: 763672 ms\n",
      "[Episode 53] Action: 0, Reward: 0.100\n",
      "[Episode 54] Latency: 763672 ms\n",
      "[Episode 54] Action: 0, Reward: 1.300\n",
      "[Episode 55] Latency: 763672 ms\n",
      "[Episode 55] Action: 17, Reward: 1.300\n",
      "[Episode 56] Latency: 763672 ms\n",
      "[Episode 56] Action: 15, Reward: 1.300\n",
      "[Episode 57] Latency: 763672 ms\n",
      "[Episode 57] Action: 2, Reward: 2.500\n",
      "[Episode 58] Latency: 763672 ms\n",
      "[Episode 58] Action: 0, Reward: 1.700\n",
      "[Episode 59] Latency: 763672 ms\n",
      "[Episode 59] Action: 0, Reward: 1.000\n",
      "[Episode 60] Latency: 763672 ms\n",
      "[Episode 60] Action: 0, Reward: 1.300\n",
      "[Episode 61] Latency: 763672 ms\n",
      "[Episode 61] Action: 0, Reward: 2.500\n",
      "[Episode 62] Latency: 763672 ms\n",
      "[Episode 62] Action: 0, Reward: 1.700\n",
      "[Episode 63] Latency: 763672 ms\n",
      "[Episode 63] Action: 0, Reward: 1.000\n",
      "[Episode 64] Latency: 763672 ms\n",
      "[Episode 64] Action: 0, Reward: 1.300\n",
      "[Episode 65] Latency: 763672 ms\n",
      "[Episode 65] Action: 6, Reward: 1.300\n",
      "[Episode 66] Latency: 763672 ms\n",
      "[Episode 66] Action: 0, Reward: 2.500\n",
      "[Episode 67] Latency: 763672 ms\n",
      "[Episode 67] Action: 0, Reward: 1.700\n",
      "[Episode 68] Latency: 763672 ms\n",
      "[Episode 68] Action: 0, Reward: 1.000\n",
      "[Episode 69] Latency: 763672 ms\n",
      "[Episode 69] Action: 0, Reward: 1.300\n",
      "[Episode 70] Latency: 763672 ms\n",
      "[Episode 70] Action: 0, Reward: 2.500\n",
      "[Episode 71] Latency: 763672 ms\n",
      "[Episode 71] Action: 1, Reward: 1.700\n",
      "[Episode 72] Latency: 763672 ms\n",
      "[Episode 72] Action: 0, Reward: 0.900\n",
      "[Episode 73] Latency: 763672 ms\n",
      "[Episode 73] Action: 0, Reward: 0.200\n",
      "[Episode 74] Latency: 763672 ms\n",
      "[Episode 74] Action: 0, Reward: 0.500\n",
      "[Episode 75] Latency: 763672 ms\n",
      "[Episode 75] Action: 0, Reward: 1.700\n",
      "[Episode 76] Latency: 763672 ms\n",
      "[Episode 76] Action: 0, Reward: 0.900\n",
      "[Episode 77] Latency: 763672 ms\n",
      "[Episode 77] Action: 0, Reward: 0.200\n",
      "[Episode 78] Latency: 763672 ms\n",
      "[Episode 78] Action: 0, Reward: 0.500\n",
      "[Episode 79] Latency: 763672 ms\n",
      "[Episode 79] Action: 0, Reward: 1.700\n",
      "[Episode 80] Latency: 763672 ms\n",
      "[Episode 80] Action: 0, Reward: 0.900\n",
      "[Episode 81] Latency: 763672 ms\n",
      "[Episode 81] Action: 0, Reward: 0.200\n",
      "[Episode 82] Latency: 763672 ms\n",
      "[Episode 82] Action: 0, Reward: 0.500\n",
      "[Episode 83] Latency: 763672 ms\n",
      "[Episode 83] Action: 0, Reward: 1.700\n",
      "[Episode 84] Latency: 763672 ms\n",
      "[Episode 84] Action: 0, Reward: 0.900\n",
      "[Episode 85] Latency: 763672 ms\n",
      "[Episode 85] Action: 18, Reward: 0.900\n",
      "[Episode 86] Latency: 763672 ms\n",
      "[Episode 86] Action: 0, Reward: 0.200\n",
      "[Episode 87] Latency: 763672 ms\n",
      "[Episode 87] Action: 0, Reward: 0.500\n",
      "[Episode 88] Latency: 763672 ms\n",
      "[Episode 88] Action: 0, Reward: 1.700\n",
      "[Episode 89] Latency: 763672 ms\n",
      "[Episode 89] Action: 0, Reward: 0.900\n",
      "[Episode 90] Latency: 763672 ms\n",
      "[Episode 90] Action: 0, Reward: 0.200\n",
      "[Episode 91] Latency: 763672 ms\n",
      "[Episode 91] Action: 0, Reward: 0.500\n",
      "[Episode 92] Latency: 763672 ms\n",
      "[Episode 92] Action: 0, Reward: 1.700\n",
      "[Episode 93] Latency: 763672 ms\n",
      "[Episode 93] Action: 0, Reward: 0.900\n",
      "[Episode 94] Latency: 763672 ms\n",
      "[Episode 94] Action: 0, Reward: 0.200\n",
      "[Episode 95] Latency: 763672 ms\n",
      "[Episode 95] Action: 0, Reward: 0.500\n",
      "[Episode 96] Latency: 763672 ms\n",
      "[Episode 96] Action: 0, Reward: 1.700\n",
      "[Episode 97] Latency: 763672 ms\n",
      "[Episode 97] Action: 0, Reward: 0.900\n",
      "[Episode 98] Latency: 763672 ms\n",
      "[Episode 98] Action: 0, Reward: 0.200\n",
      "[Episode 99] Latency: 763672 ms\n",
      "[Episode 99] Action: 0, Reward: 0.500\n"
     ]
    }
   ],
   "source": [
    "run_100episodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41a2ea94",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'caca' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcaca\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'caca' is not defined"
     ]
    }
   ],
   "source": [
    "caca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b49b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment_series(\n",
    "    param_name: str,\n",
    "    values: list[float],\n",
    "    config_vector: list[int],\n",
    "    start_ts: str,\n",
    "    end_ts: str,\n",
    "    obd_csv_path: str,\n",
    "    fixed_epsilon: float,\n",
    "    fixed_gamma: float,\n",
    "    fixed_alpha: float,\n",
    "    episodes: int = 100\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Runs a sweep over one RL hyperparameter and logs average reward over the last 20 episodes.\n",
    "    Returns a dict mapping each parameter value to its average reward.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    for val in values:\n",
    "        print(f\"\\n=== Running with {param_name} = {val:.2f} ===\")\n",
    "\n",
    "        # Set current value for the parameter being varied\n",
    "        if param_name == \"epsilon\":\n",
    "            epsilon, gamma, alpha = val, fixed_gamma, fixed_alpha\n",
    "        elif param_name == \"gamma\":\n",
    "            epsilon, gamma, alpha = fixed_epsilon, val, fixed_alpha\n",
    "        elif param_name == \"alpha\":\n",
    "            epsilon, gamma, alpha = fixed_epsilon, fixed_gamma, val\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown param_name: {param_name}\")\n",
    "\n",
    "        # Initialize agent\n",
    "        agent = QAgent(\n",
    "            action_space_size=len(ACTIONS),\n",
    "            epsilon=epsilon,\n",
    "            gamma=gamma,\n",
    "            alpha=alpha\n",
    "        )\n",
    "\n",
    "        # Train agent\n",
    "        rewards = train_q_agent(\n",
    "            agent=agent,\n",
    "            baseline_config_vector=config_vector,\n",
    "            start_ts=start_ts,\n",
    "            end_ts=end_ts,\n",
    "            obd_csv_path=obd_csv_path,\n",
    "            episodes=episodes,\n",
    "            trace=False\n",
    "        )\n",
    "\n",
    "        # Compute average of last 20 episodes\n",
    "        avg_final = round(sum(rewards[-20:]) / 20, 3)\n",
    "        results[val] = avg_final\n",
    "        print(f\"→ avg_final_reward = {avg_final:.3f}\")\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5c47db",
   "metadata": {},
   "source": [
    "#### $\\epsilon$ sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14687300",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_values = [0.05, 0.1, 0.2, 0.3]\n",
    "config_, c_vector = load_config(0) # Config0 showed the most stable behavior during first three tests\n",
    "\n",
    "results_epsilon = run_experiment_series(\n",
    "    param_name=\"epsilon\",\n",
    "    values=epsilon_values,\n",
    "    config_vector=c_vector,\n",
    "    start_ts=config_[\"test_metadata\"][0][\"ts_start\"],\n",
    "    end_ts=config_[\"test_metadata\"][0][\"ts_end\"],\n",
    "    obd_csv_path=\"./es_events/poc_and_v0_data.csv\",\n",
    "    fixed_epsilon=0.0,  ### Not used in this sweep\n",
    "    fixed_gamma=0.85,\n",
    "    fixed_alpha=0.60,\n",
    "    episodes=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b7eefd",
   "metadata": {},
   "source": [
    "#### $\\gamma$ sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bc9d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_values = [0.70, 0.85, 0.95]\n",
    "\n",
    "results_gamma = run_experiment_series(\n",
    "    param_name=\"gamma\",\n",
    "    values=gamma_values,\n",
    "    config_vector=c_vector,\n",
    "    start_ts=config_[\"test_metadata\"][0][\"ts_start\"],\n",
    "    end_ts=config_[\"test_metadata\"][0][\"ts_end\"],\n",
    "    obd_csv_path=\"./es_events/poc_and_v0_data.csv\",\n",
    "    fixed_epsilon=0.30,  # best from last sweep\n",
    "    fixed_gamma=0.0,     # ignored in sweep\n",
    "    fixed_alpha=0.60,\n",
    "    episodes=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a8df83",
   "metadata": {},
   "source": [
    "#### $\\alpha$ sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5669b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_values = [0.3, 0.5, 0.7]\n",
    "\n",
    "results_alpha = run_experiment_series(\n",
    "    param_name=\"alpha\",\n",
    "    values=alpha_values,\n",
    "    config_vector=c_vector,\n",
    "    start_ts=config_[\"test_metadata\"][0][\"ts_start\"],\n",
    "    end_ts=config_[\"test_metadata\"][0][\"ts_end\"],\n",
    "    obd_csv_path=\"./es_events/poc_and_v0_data.csv\",\n",
    "    fixed_epsilon=0.30,\n",
    "    fixed_gamma=0.70,\n",
    "    fixed_alpha=0.0,  # not used in this sweep\n",
    "    episodes=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373eab85",
   "metadata": {},
   "source": [
    "### Unit tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28b0112",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Helper functions\n",
    "\n",
    "def generate_random_config() -> dict:\n",
    "    config = {\n",
    "        \"obd_parameters\": [],\n",
    "        \"global_settings\": {}\n",
    "    }\n",
    "\n",
    "    for pid in ACTIVE_PIDS:\n",
    "        enabled = random.choice([True, False])\n",
    "        strategy = random.choice(STRATEGY_LIST)\n",
    "        config[\"obd_parameters\"].append({\n",
    "            \"pid\": pid,\n",
    "            \"enabled\": enabled,\n",
    "            \"strategy\": strategy\n",
    "        })\n",
    "\n",
    "    config[\"global_settings\"] = {\n",
    "        \"min_time\": random.choice([5, 10, 30, 60, 120, 300]),\n",
    "        \"send_period\": random.choice([10, 30, 60, 120, 300]),\n",
    "        \"min_saved_records\": random.randint(1, 10)\n",
    "    }\n",
    "\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160362d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise StopIteration(\"Manual testing block below. Execution stopped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d3d8d8",
   "metadata": {},
   "source": [
    "#### Encode / decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff4d0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"obd_parameters\": [\n",
    "        {\"pid\": 12, \"enabled\": True, \"strategy\": \"on_change\"},\n",
    "        {\"pid\": 13, \"enabled\": True, \"strategy\": \"monitoring\"},\n",
    "        {\"pid\": 39, \"enabled\": False, \"strategy\": \"hysteresis\"},\n",
    "    ],\n",
    "    \"global_settings\": {\n",
    "        \"min_time\": 10,\n",
    "        \"send_period\": 60,\n",
    "        \"min_saved_records\": 1\n",
    "    }\n",
    "}\n",
    "\n",
    "vec = encode_config(config)\n",
    "decoded = decode_config(vec)\n",
    "assert config == decoded  # Should pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c47793",
   "metadata": {},
   "source": [
    "#### Apply action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04989f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_apply_action_on_all_ids():\n",
    "    base_config = generate_random_config()\n",
    "    baseline_vector = encode_config(base_config)\n",
    "    print(\"Base Config:\", decode_config(baseline_vector))\n",
    "\n",
    "    n_pids = len(ACTIVE_PIDS)\n",
    "    total_actions = len(ACTIONS)\n",
    "\n",
    "    for action_id in range(total_actions):\n",
    "        print(f\"\\n--- Testing action {action_id} ---\")\n",
    "        new_vector, trace = apply_action(baseline_vector, action_id, trace=True, baseline=baseline_vector)\n",
    "\n",
    "        # Assert same length\n",
    "        assert len(new_vector) == len(baseline_vector), f\"Vector length changed for action {action_id}\"\n",
    "\n",
    "        # Assert valid strategy index\n",
    "        for i in range(n_pids):\n",
    "            strategy_idx = new_vector[i * 3 + 2]\n",
    "            assert 0 <= strategy_idx < len(STRATEGY_LIST), f\"Invalid strategy index {strategy_idx} after action {action_id}\"\n",
    "\n",
    "        # Global settings sanity check\n",
    "        g_base = n_pids * 3\n",
    "        assert new_vector[g_base] >= 1, \"min_time below 1\"\n",
    "        assert new_vector[g_base + 1] >= 1, \"send_period below 1\"\n",
    "        assert 1 <= new_vector[g_base + 2] <= 10, \"min_saved_records out of bounds\"\n",
    "\n",
    "        print(\"Trace:\", trace)\n",
    "        print(\"✅ Passed all assertions for action\", action_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca352ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_apply_action_on_all_ids()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3daff572",
   "metadata": {},
   "source": [
    "#### Q-Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aedb1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_qagent_basic():\n",
    "    agent = QAgent(action_space_size=13, epsilon=0.0)  # deterministic\n",
    "    state = encode_config({\n",
    "        \"obd_parameters\": [\n",
    "            {\"pid\": 12, \"enabled\": True, \"strategy\": \"on_change\"},\n",
    "            {\"pid\": 13, \"enabled\": True, \"strategy\": \"on_change\"},\n",
    "            {\"pid\": 39, \"enabled\": True, \"strategy\": \"on_change\"}\n",
    "        ],\n",
    "        \"global_settings\": {\"min_time\": 10, \"send_period\": 60, \"min_saved_records\": 1}\n",
    "    })\n",
    "\n",
    "    action = agent.select_action(state)\n",
    "    next_state, _ = apply_action(state, action, baseline=state)\n",
    "    reward = 2.5\n",
    "    agent.update(state, action, reward, next_state)\n",
    "\n",
    "    # Assert Q-value updated\n",
    "    state_key = tuple(state)\n",
    "    assert action < len(agent.q_table[state_key]), \"Q-table entry missing\"\n",
    "    q_value = agent.q_table[state_key][action]\n",
    "    assert q_value != 0.0, \"Q-value not updated\"\n",
    "    print(f\"✅ Q-table updated: Q[state][{action}] = {q_value:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70372d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_qagent_basic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83df0d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_qagent_training():\n",
    "    baseline_config = {\n",
    "        \"obd_parameters\": [\n",
    "            {\"pid\": 12, \"enabled\": True, \"strategy\": \"monitoring\"},\n",
    "            {\"pid\": 13, \"enabled\": True, \"strategy\": \"monitoring\"},\n",
    "            {\"pid\": 39, \"enabled\": True, \"strategy\": \"monitoring\"}\n",
    "        ],\n",
    "        \"global_settings\": {\"min_time\": 300, \"send_period\": 120, \"min_saved_records\": 1}\n",
    "    }\n",
    "\n",
    "    baseline_vector = encode_config(baseline_config)\n",
    "    agent = QAgent(action_space_size=13, epsilon=0.3)\n",
    "\n",
    "    rewards = train_q_agent(\n",
    "        agent,\n",
    "        baseline_vector,\n",
    "        start_ts=\"2025-05-16T06:40:38Z\",\n",
    "        end_ts=\"2025-05-17T23:59:00Z\"  ,\n",
    "        freq_csv_path=\"../../data_proc/csv_data/qa_device/frequencies.csv\",\n",
    "        obd_csv_path=\"../../data_proc/csv_data/qa_device/obd_export.csv\",\n",
    "        episodes=5,\n",
    "        trace=True\n",
    "    )\n",
    "\n",
    "    assert len(rewards) == 5, \"Incorrect number of training episodes\"\n",
    "    assert all(isinstance(r, (float, int)) for r in rewards), \"Non-numeric reward detected\"\n",
    "    print(\"✅ Training rewards:\", rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ffeb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_qagent_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c28be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training\n",
    "\n",
    "baseline_config = {\n",
    "    \"obd_parameters\": [\n",
    "        {\"pid\": 12, \"enabled\": True, \"strategy\": \"monitoring\"},\n",
    "        {\"pid\": 13, \"enabled\": True, \"strategy\": \"monitoring\"},\n",
    "        {\"pid\": 39, \"enabled\": True, \"strategy\": \"monitoring\"}\n",
    "    ],\n",
    "    \"global_settings\": {\"min_time\": 300, \"send_period\": 120, \"min_saved_records\": 1}\n",
    "}\n",
    "\n",
    "baseline_vector = encode_config(baseline_config)\n",
    "\n",
    "agent = QAgent(action_space_size=13, epsilon=0.3)\n",
    "rewards = train_q_agent(\n",
    "    agent,\n",
    "    baseline_vector,\n",
    "    start_ts=\"2025-05-23T13:55:00Z\",\n",
    "    end_ts=\"2025-05-23T14:07:00Z\",\n",
    "    freq_csv_path=\"frequencies.csv\",\n",
    "    obd_csv_path=\"obd_export.csv\"\n",
    ")\n",
    "\n",
    "plot_rewards(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883353b0",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27132366",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Implementation references**\n",
    "\n",
    "- OpenAI Spinning Up: https://spinningup.openai.com\n",
    "Although it focuses more on policy-gradient methods, it gives good context on where Q-learning fits in the broader RL ecosystem.\n",
    "\n",
    "- RL Course by David Silver (DeepMind)\n",
    "Lectures 4–6 cover model-free methods, including Q-Learning.\n",
    "\n",
    "- Towards Data Science\n",
    "https://towardsdatascience.com/reinforcement-learning-explained-visually-part-4-q-learning-step-by-step-b65efb731d3e/\n",
    "\n",
    "\n",
    "\n",
    "**Academic references**\n",
    "\n",
    "1. Watkins, C.J.C.H., & Dayan, P. (1992)\n",
    "   Q-learning: https://link.springer.com/article/10.1007/BF00992698\n",
    "\n",
    "   \n",
    "2. Sutton, R. S., & Barto, A. G. (2018)\n",
    "    Reinforcement Learning: An Introduction (2nd Edition)\n",
    "    Chapter 6 covers Q-Learning in depth.\n",
    "    http://incompleteideas.net/book/the-book-2nd.html "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aau",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

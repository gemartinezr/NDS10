{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4db09d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import random\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316ccab3",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9687088",
   "metadata": {},
   "source": [
    "**Baseline config** initial state:  (used in production)   \n",
    "3 PIDs (engine load, rpm, speed) set to monitoring, with 300 s Min Period (distance, angle, speed delta are not used), min saved records = 1, send period = 120.\n",
    "\n",
    "**Worst config**  initial state (from my own experience)  \n",
    "3 PIDs (engine load, rpm, speed) set to on_change, with 300 s Min Period (distance, angle, speed delta are not used), min saved records = 1, send period = 120.\n",
    "\n",
    "**Test conditions**\n",
    "1. \"Vehicle trip\" (test24 since  I am leaning to 12 minutes, or shorter test periods where it's just values increasing monotonically)\n",
    "2.  Sleep, battery, GPS precision and network mode (home, network, unkown) are assumed optimal. \n",
    "\n",
    "**Actions**  \n",
    "1. $p_{i_0}$----PID (integer identifier for PID, e.g. 12 = RPM)\n",
    "2. $p_{i_1}$----enable PID $i$   (0 or 1)\n",
    "3. $p_{i_2}$----change data acquisition strategy for PID $i$  *(monitoring, on_change, hysteresis, delta_change)*  \n",
    "4. $d_0$------increase/decrease min_time  (time the device lets pass before querying PID values)     *(seconds, 0 disables it)*  \n",
    "5. $d_1$------increase/decrease min_saved (number of records the device will accumulate before sending) *(int, 0 disables it)*  \n",
    "6. $d_2$------increase/decrease send_time  (time the device lets pass before attempting to send a new record)  *(seconds, 0 disables it)*  \n",
    "7. $d_3$------reset baseline  (Device)   *reset to a known stable configuration*  (0,1)\n",
    " \n",
    "\n",
    "For example, considering 3 different PIDs a state $S$ is defined like: \n",
    "\n",
    "$S = (p_{00}, p_{01}, p_{02}, p_{10}, p_{11}, p_{12}, p_{20}, p_{21}, p_{22}, d_0, d_1, d_2, d_3)$\n",
    "\n",
    "So for 3 PIDs, the $S$ vector is of size $13$, and in general:  $3n+ 4$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0a4641",
   "metadata": {},
   "source": [
    "### PID configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf3a675",
   "metadata": {},
   "outputs": [],
   "source": [
    "STRATEGY_MAP = {\n",
    "    \"monitoring\": 0,\n",
    "    \"on_change\": 1,\n",
    "    \"hysteresis\": 2,\n",
    "    \"on_delta_change\": 3 # on_enter, on_exit, on_both are omitted to simplify the model.\n",
    "}\n",
    "\n",
    "REVERSE_STRATEGY_MAP = {v: k for k, v in STRATEGY_MAP.items()}\n",
    "STRATEGY_LIST = list(STRATEGY_MAP.keys())\n",
    "\n",
    "\n",
    "#####\n",
    "\n",
    "ACTIVE_PIDS = [12, 13, 39]  # Extend this to more PIDs as needed\n",
    "n_pids = len(ACTIVE_PIDS)\n",
    "\n",
    "PID_PRECISION = {\n",
    "    12: 100,\n",
    "    13: 5,\n",
    "    39: 5,\n",
    "    28: 2,\n",
    "    38: 3,\n",
    "    31: 3\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f36e25",
   "metadata": {},
   "source": [
    "### State encoder / decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdf41bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_config(config: dict) -> list:\n",
    "    \"\"\"\n",
    "    Converts a human-readable config dict into a state vector.\n",
    "    \"\"\"\n",
    "    vector = []\n",
    "\n",
    "    # Encode OBD parameters\n",
    "    for param in config.get(\"obd_parameters\", []):\n",
    "        pid = param.get(\"pid\", 0)\n",
    "        enabled = 1 if param.get(\"enabled\", False) else 0\n",
    "        strategy_idx = STRATEGY_MAP.get(param.get(\"strategy\", \"monitoring\"), 0)\n",
    "        vector.extend([pid, enabled, strategy_idx])\n",
    "\n",
    "    # Global settings\n",
    "    global_ = config.get(\"global_settings\", {})\n",
    "    vector.extend([\n",
    "        global_.get(\"min_time\", 10),\n",
    "        global_.get(\"send_period\", 10),\n",
    "        global_.get(\"min_saved_records\", 1)\n",
    "    ])\n",
    "\n",
    "    return vector\n",
    "\n",
    "\n",
    "def decode_config(vector: list) -> dict:\n",
    "    \"\"\"\n",
    "    Converts a state vector into a human-readable config dict.\n",
    "    \"\"\"\n",
    "    config = {\"obd_parameters\": [], \"global_settings\": {}}\n",
    "    n_pids = len(ACTIVE_PIDS)\n",
    "\n",
    "    # Decode OBD parameter block\n",
    "    for i in range(n_pids):\n",
    "        base = i * 3\n",
    "        pid, enabled, strategy_idx = vector[base:base + 3]\n",
    "        config[\"obd_parameters\"].append({\n",
    "            \"pid\": pid,\n",
    "            \"enabled\": bool(enabled),\n",
    "            \"strategy\": REVERSE_STRATEGY_MAP.get(strategy_idx, \"monitoring\")\n",
    "        })\n",
    "\n",
    "    # Decode global settings\n",
    "    g_base = n_pids * 3\n",
    "    config[\"global_settings\"] = {\n",
    "        \"min_time\": vector[g_base],\n",
    "        \"send_period\": vector[g_base + 1],\n",
    "        \"min_saved_records\": vector[g_base + 2]\n",
    "    }\n",
    "\n",
    "    return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0300928",
   "metadata": {},
   "source": [
    "### Actions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "804282f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pids = len(ACTIVE_PIDS)\n",
    "g_base = n_pids * 3  # index where global settings start\n",
    "\n",
    "# Per-PID toggle: action_id in 0 to n_pids - 1\n",
    "# Per-PID strategy: action_id in n_pids to 2 * n_pids - 1\n",
    "# Global actions: fixed offsets starting from 2 * n_pids\n",
    "\n",
    "def apply_action(state: list[int], action_id: int, trace: bool = True, baseline: list[int] = None) -> tuple[list[int], dict]:\n",
    "    \"\"\"\n",
    "    Applies a single action to the config vector and returns (new_state, trace_dict).\n",
    "    The state layout is: [pid, enabled, strategy_idx] * N + [min_time, send_period, min_saved_records]\n",
    "    \"\"\"\n",
    "    new_vector = deepcopy(state)\n",
    "    trace_info = {}\n",
    "\n",
    "    n_pids = len(ACTIVE_PIDS)\n",
    "    g_base = n_pids * 3\n",
    "\n",
    "    def log(msg):\n",
    "        if trace:\n",
    "            print(f\"[apply_action] {msg}\")\n",
    "        trace_info[\"change\"] = msg\n",
    "\n",
    "    # PID toggle: action 0 to N-1\n",
    "    if action_id < n_pids:\n",
    "        idx = action_id\n",
    "        base = idx * 3\n",
    "        new_vector[base + 1] = 1 - new_vector[base + 1]\n",
    "        log(f\"Toggled PID {new_vector[base]} enable → {new_vector[base + 1]}\")\n",
    "        return new_vector, trace_info\n",
    "\n",
    "    # PID strategy cycle: action N to 2N-1\n",
    "    elif action_id < 2 * n_pids:\n",
    "        idx = action_id - n_pids\n",
    "        base = idx * 3\n",
    "        current = new_vector[base + 2]\n",
    "        new_vector[base + 2] = (current + 1) % len(STRATEGY_LIST)\n",
    "        log(f\"PID {new_vector[base]} strategy: {STRATEGY_LIST[current]} → {STRATEGY_LIST[new_vector[base + 2]]}\")\n",
    "        return new_vector, trace_info\n",
    "\n",
    "    # Global actions start at 2 * n_pids\n",
    "    global_actions = {\n",
    "        0: (\"min_time\", g_base, 5, 300),\n",
    "        1: (\"min_time\", g_base, -5, 1),\n",
    "        2: (\"send_period\", g_base + 1, 5, 300),\n",
    "        3: (\"send_period\", g_base + 1, -5, 1),\n",
    "        4: (\"min_saved_records\", g_base + 2, 1, 10),\n",
    "        5: (\"min_saved_records\", g_base + 2, -1, 1),\n",
    "    }\n",
    "\n",
    "    global_action_id = action_id - 2 * n_pids\n",
    "\n",
    "    if global_action_id in global_actions:\n",
    "        label, index, delta, limit = global_actions[global_action_id]\n",
    "        old_val = new_vector[index]\n",
    "        new_val = old_val + delta\n",
    "        new_val = max(min(new_val, max(limit, old_val)), min(limit, old_val))\n",
    "        new_vector[index] = new_val\n",
    "        log(f\"{label} changed from {old_val} → {new_val}\")\n",
    "        return new_vector, trace_info\n",
    "\n",
    "    # Reset to baseline\n",
    "    if global_action_id == 6 and baseline is not None:\n",
    "        new_vector = deepcopy(baseline)\n",
    "        log(\"Reset to baseline configuration\")\n",
    "        return new_vector, trace_info\n",
    "\n",
    "    log(f\"No change for action_id={action_id}\")\n",
    "    return new_vector, trace_info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c96c8ac",
   "metadata": {},
   "source": [
    "### Reward function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b52f8ac",
   "metadata": {},
   "source": [
    "reward = latency_score + sum(pid_scores)\n",
    "\n",
    "\n",
    "compute_reward() inputs: \n",
    "    latency_ms: int\n",
    "    pid_data_list: list of dictionaries\n",
    "\n",
    "For example:    \n",
    "    latency_ms = 1850\n",
    "```json\n",
    "    pid_data_list = [\n",
    "        {\n",
    "            \"pid\": 12,\n",
    "            \"values\": [1500, 1500, 1500],\n",
    "            \"strategy\": \"on_change\",\n",
    "            \"precision\": 100,\n",
    "            \"valid_range\": (800, 6000)\n",
    "        },\n",
    "        // rest of the PIDs\n",
    "]```\n",
    "\n",
    "**Precision**\n",
    "    This defines how much a value must change to be considered “meaningful.”\n",
    "    Even if all values are valid, we may want to ignore tiny fluctuations (e.g., RPM changes by 1 unit) and log only useful variation (e.g., ±100 RPM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037d3c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def latency_score(latency_ms: int) -> float:\n",
    "    \"\"\"Scores latency from 1.0 (fast) to -0.5 (excessive delay).\"\"\"\n",
    "    if latency_ms <= 500:\n",
    "        return 1.0\n",
    "    elif latency_ms <= 5000:\n",
    "        return 1 - (latency_ms - 500) / 4500\n",
    "    else:\n",
    "        return -0.5\n",
    "    \n",
    "\n",
    "def compute_pid_scores(pid_data_list: list[dict]) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Computes per-PID scores based on values, strategy, precision, and valid_range.\n",
    "    Filters out 'pid' before calling the scoring function.\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "\n",
    "    for entry in pid_data_list:\n",
    "        score = data_quality_score(\n",
    "            values=entry[\"values\"],\n",
    "            strategy=entry[\"strategy\"],\n",
    "            precision=PID_PRECISION.get(entry[\"pid\"], 1),\n",
    "            valid_range=entry[\"valid_range\"]\n",
    "        )\n",
    "        scores.append({\n",
    "            \"pid\": entry[\"pid\"],\n",
    "            \"score\": round(score, 3)\n",
    "        })\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "def data_quality_score(\n",
    "    values: list[float],\n",
    "    strategy: str,\n",
    "    precision: float,\n",
    "    valid_range: tuple[float, float],\n",
    ") -> float:\n",
    "    \"\"\"Scores how well a single PID was logged based on value range, variation, and strategy fit.\"\"\"\n",
    "    if not values:\n",
    "        return -1.0\n",
    "\n",
    "    # 1. Range check\n",
    "    if not all(valid_range[0] <= v <= valid_range[1] for v in values):\n",
    "        return -1.0\n",
    "\n",
    "    # 2. Variation check\n",
    "    variation = max(values) - min(values)\n",
    "    significant = variation >= precision\n",
    "\n",
    "    # 3. Strategy match evaluation\n",
    "    if strategy == \"on_change\":\n",
    "        return 1.0 if not significant else 0.5\n",
    "    elif strategy == \"on_delta_change\":\n",
    "        return 1.0 if significant else -0.5\n",
    "    elif strategy == \"hysteresis\":\n",
    "        return 0.8 if significant else 0.2\n",
    "    elif strategy == \"monitoring\":\n",
    "        return 0.6 if significant else -0.2\n",
    "\n",
    "    return 0.0  # Fallback for unrecognized strategies\n",
    "\n",
    "\n",
    "def compute_average_quality(pid_data_list: list[dict]) -> float:\n",
    "    \"\"\"\n",
    "    Computes the total quality score by summing each PID's score.\n",
    "    Each PID contributes equally to the total reward (no weighting).\n",
    "    \"\"\"\n",
    "    scores = [data_quality_score(**entry) for entry in pid_data_list]\n",
    "    return round(sum(scores), 3) if scores else 0.0\n",
    "\n",
    "\n",
    "def clip_reward(score, min_val=-1.0, max_val=3.0):\n",
    "    \"\"\"Clipping reward is an optional step\n",
    "    it helps reduce the traininf dominance of extreme values\n",
    "    and when a tighter contol over agent learning rate stability is needed.\"\"\"\n",
    "    return max(min(score, max_val), min_val)\n",
    "\n",
    "\n",
    "def compute_reward(latency_ms: int, pid_data_list: list[dict]) -> float:\n",
    "    \"\"\"\n",
    "    Composite reward:\n",
    "    - Latency is scored once for the entire configuration\n",
    "    - Each PID adds to or subtracts from the total score\n",
    "    \"\"\"\n",
    "    latency = latency_score(latency_ms)\n",
    "    quality_sum = compute_average_quality(pid_data_list)\n",
    "    return round(latency + quality_sum, 3)\n",
    "\n",
    "\n",
    "def compute_reward_with_details(latency_ms: int, pid_data_list: list[dict]) -> tuple[float, dict]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    - Total reward score\n",
    "    - A breakdown dictionary with latency score and individual PID scores\n",
    "    \"\"\"\n",
    "    for entry in pid_data_list:\n",
    "        entry[\"precision\"] = PID_PRECISION.get(entry[\"pid\"], 1)\n",
    "        \n",
    "    latency = latency_score(latency_ms)\n",
    "    pid_scores = compute_pid_scores(pid_data_list)\n",
    "    total_pid_score = sum(entry[\"score\"] for entry in pid_scores)\n",
    "\n",
    "    total_reward = round(latency + total_pid_score, 3)\n",
    "\n",
    "    breakdown = {\n",
    "        \"latency_ms\": latency_ms,\n",
    "        \"latency_score\": round(latency, 3),\n",
    "        \"pid_scores\": pid_scores,\n",
    "        \"total_pid_score\": round(total_pid_score, 3),\n",
    "    }\n",
    "\n",
    "    return total_reward, breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bf755ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flag_latency_outliers(freq_df: pd.DataFrame, threshold_ms: int = 7_200_000) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds a boolean column 'is_latency_outlier' to the input DataFrame,\n",
    "    where True indicates latency greater than the given threshold (default: 2 hours).\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert timestamps\n",
    "    freq_df[\"ts_recorded\"] = pd.to_datetime(freq_df[\"ts_recorded\"])\n",
    "    freq_df[\"ts_uploaded\"] = pd.to_datetime(freq_df[\"ts_uploaded\"])\n",
    "\n",
    "    # Compute latency in milliseconds\n",
    "    freq_df[\"latency_ms\"] = (freq_df[\"ts_uploaded\"] - freq_df[\"ts_recorded\"]).dt.total_seconds() * 1000\n",
    "\n",
    "    # Flag all rows with latency above the threshold\n",
    "    freq_df[\"is_latency_outlier\"] = freq_df[\"latency_ms\"] > threshold_ms\n",
    "\n",
    "    return freq_df\n",
    "\n",
    "\n",
    "def get_latency(start_ts, end_ts, method='median'):\n",
    "    freq_df = pd.read_csv('../../data_proc/csv_data/qa_device/frequencies.csv', low_memory=False)\n",
    "    freq_df.drop('Unnamed: 0', inplace=True, axis=1)\n",
    "\n",
    "    # Parse timestamps\n",
    "    freq_df[\"ts_recorded\"] = pd.to_datetime(freq_df[\"ts_recorded\"])\n",
    "    freq_df[\"ts_uploaded\"] = pd.to_datetime(freq_df[\"ts_uploaded\"])\n",
    "\n",
    "    # Filter within time range\n",
    "    mask = (freq_df[\"ts_recorded\"] >= pd.to_datetime(start_ts)) & (freq_df[\"ts_recorded\"] <= pd.to_datetime(end_ts))\n",
    "    filtered_df = freq_df[mask].copy()\n",
    "\n",
    "    if filtered_df.empty:\n",
    "        return None  # or raise an exception if preferred\n",
    "\n",
    "    # Compute latency in ms\n",
    "    filtered_df[\"latency_ms\"] = (filtered_df[\"ts_uploaded\"] - filtered_df[\"ts_recorded\"]).dt.total_seconds() * 1000\n",
    "\n",
    "    if method == 'median':\n",
    "        return filtered_df[\"latency_ms\"].median()\n",
    "    elif method == 'mean':\n",
    "        return filtered_df[\"latency_ms\"].mean()\n",
    "    else:\n",
    "        raise ValueError(\"method must be 'median' or 'mean'\")\n",
    "\n",
    "\n",
    "# Define a field-to-PID mapping\n",
    "FIELD_TO_PID = {\n",
    "    \"obd.rpm.value\": 12,\n",
    "    \"obd.speed.value\": 13,\n",
    "    \"obd.fuel_level.value\": 28,\n",
    "    \"obd.coolant_temp.value\": 38,\n",
    "    \"obd.engine_load.value\": 39,\n",
    "    \"obd.intake_temp.value\": 20,\n",
    "    \"obd.maf.value\": 21,\n",
    "    \"obd.throttle_pos.value\": 41,\n",
    "    \"obd.ambient_air_temp.value\": 131,\n",
    "    \"obd.distance_since_codes_clear.value\": 31,\n",
    "    \"obd.time_since_codes_cleared.value\": 47,\n",
    "    # Add more as needed...\n",
    "}\n",
    "\n",
    "\n",
    "def extract_pid_statistics(obd_df: pd.DataFrame, start_ts: str, end_ts: str) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Extracts per-PID reward inputs (PID ID, values, precision, valid range)\n",
    "    from an obd_export dataframe filtered by timestamp range.\n",
    "    Returns a list of dictionaries ready for reward scoring.\n",
    "    \"\"\"\n",
    "    # Filter by time window\n",
    "    obd_df[\"@ts\"] = pd.to_datetime(obd_df[\"@ts\"])\n",
    "    mask = (obd_df[\"@ts\"] >= pd.to_datetime(start_ts)) & (obd_df[\"@ts\"] <= pd.to_datetime(end_ts))\n",
    "    obd_df = obd_df[mask].copy()\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for field, pid in FIELD_TO_PID.items():\n",
    "        if field not in obd_df.columns:\n",
    "            continue\n",
    "\n",
    "        values = obd_df[field].dropna().astype(float).tolist()\n",
    "        if not values:\n",
    "            continue\n",
    "\n",
    "        vmin, vmax = min(values), max(values)\n",
    "        vrange = vmax - vmin\n",
    "\n",
    "        precision = round(vrange * 0.1, 3) if vrange > 0 else 1.0\n",
    "        valid_range = (vmin - vrange * 0.1, vmax + vrange * 0.1)\n",
    "\n",
    "        results.append({\n",
    "            \"pid\": pid,\n",
    "            \"values\": values,\n",
    "            \"precision\": precision,\n",
    "            \"valid_range\": valid_range\n",
    "        })\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7fcb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_pid_statistics(pid_data_list: list[dict]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns a DataFrame summarizing each PID's stats (excluding raw values).\n",
    "    \"\"\"\n",
    "    summary = []\n",
    "\n",
    "    for entry in pid_data_list:\n",
    "        values = entry[\"values\"]\n",
    "        pid_summary = {\n",
    "            \"PID\": entry[\"pid\"],\n",
    "            \"Count\": len(values),\n",
    "            \"Min\": min(values) if values else None,\n",
    "            \"Max\": max(values) if values else None,\n",
    "            \"Precision\": PID_PRECISION.get(entry[\"pid\"], 1),\n",
    "            \"Valid Range\": entry[\"valid_range\"],\n",
    "        }\n",
    "        summary.append(pid_summary)\n",
    "\n",
    "    return pd.DataFrame(summary).sort_values(by=\"PID\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e563591f",
   "metadata": {},
   "source": [
    "### Q-Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf3979bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAgent:\n",
    "    def __init__(self, action_space_size, epsilon=0.2, alpha=0.5, gamma=0.9):\n",
    "        self.q_table = defaultdict(lambda: [0.0] * action_space_size)\n",
    "        self.epsilon = epsilon  # exploration rate\n",
    "        self.alpha = alpha      # learning rate\n",
    "        self.gamma = gamma      # discount factor\n",
    "        self.action_space_size = action_space_size\n",
    "\n",
    "    def select_action(self, state_vector: list[int]) -> int:\n",
    "        state_key = tuple(state_vector)\n",
    "\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, self.action_space_size - 1)  # explore\n",
    "        else:\n",
    "            q_values = self.q_table[state_key]\n",
    "            return int(q_values.index(max(q_values)))  # exploit\n",
    "\n",
    "    def update(self, state: list[int], action: int, reward: float, next_state: list[int]):\n",
    "        state_key = tuple(state)\n",
    "        next_state_key = tuple(next_state)\n",
    "\n",
    "        old_value = self.q_table[state_key][action]\n",
    "        next_max = max(self.q_table[next_state_key])\n",
    "\n",
    "        # Q-learning update rule\n",
    "        new_value = old_value + self.alpha * (reward + self.gamma * next_max - old_value)\n",
    "        self.q_table[state_key][action] = new_value\n",
    "\n",
    "    def decay_epsilon(self, decay_rate=0.99):\n",
    "        self.epsilon *= decay_rate\n",
    "\n",
    "    def save_q_table(self, path='q_table.json'):\n",
    "        # Convert keys to strings for JSON serialization\n",
    "        json_q = {str(k): v for k, v in self.q_table.items()}\n",
    "        with open(path, 'w') as f:\n",
    "            json.dump(json_q, f, indent=2)\n",
    "\n",
    "    def load_q_table(self, path='q_table.json'):\n",
    "        with open(path, 'r') as f:\n",
    "            json_q = json.load(f)\n",
    "        self.q_table = defaultdict(lambda: [0.0] * self.action_space_size)\n",
    "        for k, v in json_q.items():\n",
    "            self.q_table[tuple(eval(k))] = v\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb272bc6",
   "metadata": {},
   "source": [
    "### Vehicle trip simulation / Q-Agent training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d695c6c",
   "metadata": {},
   "source": [
    "1. Start from a known config (state)\n",
    "2. Select an action with ε-greedy\n",
    "3. Applies it to get next_state\n",
    "4. Run run_vehicle_sim() to compute reward\n",
    "5. Update Q-table\n",
    "6. Repeat for N episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9cfa7f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_vehicle_sim(config_vector, start_ts, end_ts,\n",
    "                      freq_csv_path, obd_csv_path,\n",
    "                      trace=False):\n",
    "    \"\"\"\n",
    "    Replays a config against CSV logs and computes the reward.\n",
    "    \"\"\"\n",
    "    # Load logs\n",
    "    freq_df = pd.read_csv(freq_csv_path, low_memory=False)\n",
    "    freq_df.drop('Unnamed: 0', inplace=True, axis=1)\n",
    "    freq_df[\"ts_recorded\"] = pd.to_datetime(freq_df[\"ts_recorded\"])\n",
    "    freq_df[\"ts_uploaded\"] = pd.to_datetime(freq_df[\"ts_uploaded\"])\n",
    "\n",
    "    obd_df = pd.read_csv(obd_csv_path, low_memory=False)\n",
    "    obd_df.drop('Unnamed: 0', inplace=True, axis=1)\n",
    "    obd_df[\"@ts\"] = pd.to_datetime(obd_df[\"@ts\"])\n",
    "\n",
    "    # Filter time window\n",
    "    start_ts = pd.to_datetime(start_ts)\n",
    "    end_ts = pd.to_datetime(end_ts)\n",
    "    freq_df = freq_df[(freq_df[\"ts_recorded\"] >= start_ts) & (freq_df[\"ts_recorded\"] <= end_ts)]\n",
    "    obd_df = obd_df[(obd_df[\"@ts\"] >= start_ts) & (obd_df[\"@ts\"] <= end_ts)]\n",
    "\n",
    "    if trace:\n",
    "        print(f\"[Replay] Freq records: {len(freq_df)} | OBD records: {len(obd_df)}\")\n",
    "\n",
    "\n",
    "    threshold_ms = 2 * 60 * 60 * 1000  # 2 hours in ms\n",
    "\n",
    "    # Compute latency column\n",
    "    freq_df[\"latency_ms\"] = (freq_df[\"ts_uploaded\"] - freq_df[\"ts_recorded\"]).dt.total_seconds() * 1000\n",
    "    # Flag outliers\n",
    "    freq_df[\"is_latency_outlier\"] = freq_df[\"latency_ms\"] > threshold_ms\n",
    "    # Replace outliers with max(valid_latencies)\n",
    "    valid_latencies = freq_df.loc[~freq_df[\"is_latency_outlier\"], \"latency_ms\"]\n",
    "    if not valid_latencies.empty:\n",
    "        max_valid_latency = valid_latencies.max()\n",
    "        freq_df.loc[freq_df[\"is_latency_outlier\"], \"latency_ms\"] = max_valid_latency\n",
    "        if trace:\n",
    "            outlier_count = freq_df[\"is_latency_outlier\"].sum()\n",
    "            print(f\"[Replay] {outlier_count} outlier(s) replaced with max(valid_latency): {max_valid_latency:.1f} ms\")\n",
    "\n",
    "    # Estimate latency\n",
    "    freq_df[\"latency_ms\"] = (freq_df[\"ts_uploaded\"] - freq_df[\"ts_recorded\"]).dt.total_seconds() * 1000\n",
    "    median_latency = freq_df[\"latency_ms\"].median()\n",
    "    \n",
    "    if trace:\n",
    "        print(f\"[Replay] Median latency: {median_latency:.1f} ms\")\n",
    "\n",
    "    # Extract PID statistics\n",
    "    pid_data_list = extract_pid_statistics(obd_df, start_ts, end_ts)\n",
    "\n",
    "    # Inject strategy from config\n",
    "    decoded = decode_config(config_vector)\n",
    "    pid_by_id = {p[\"pid\"]: p[\"strategy\"] for p in decoded[\"obd_parameters\"]}\n",
    "    for p in pid_data_list:\n",
    "        p[\"strategy\"] = pid_by_id.get(p[\"pid\"], \"monitoring\")\n",
    "\n",
    "    # Compute reward\n",
    "    reward, breakdown = compute_reward_with_details(median_latency, pid_data_list)\n",
    "    if trace:\n",
    "        print(f\"[Replay] Reward: {reward:.3f} → Breakdown: {breakdown}\")\n",
    "\n",
    "    return reward, breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5f5bbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_q_agent(\n",
    "    agent,\n",
    "    baseline_config_vector,\n",
    "    start_ts,\n",
    "    end_ts,\n",
    "    freq_csv_path,\n",
    "    obd_csv_path,\n",
    "    episodes=30,\n",
    "    trace=True\n",
    "):\n",
    "    state = baseline_config_vector.copy()\n",
    "    rewards_per_episode = []\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        action = agent.select_action(state)\n",
    "        next_state, trace_info = apply_action(state, action, trace=False, baseline=baseline_config_vector)\n",
    "\n",
    "        try:\n",
    "            reward, _ = run_vehicle_sim(\n",
    "                config_vector=next_state,\n",
    "                start_ts=start_ts,\n",
    "                end_ts=end_ts,\n",
    "                freq_csv_path=freq_csv_path,\n",
    "                obd_csv_path=obd_csv_path,\n",
    "                trace=False\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"[Episode {ep}] Simulation failed: {e}\")\n",
    "            reward = -1.0  # penalize invalid config\n",
    "\n",
    "        # Update Q-table\n",
    "        agent.update(state, action, reward, next_state)\n",
    "\n",
    "        # Log reward and prepare for next episode\n",
    "        rewards_per_episode.append(reward)\n",
    "        state = next_state\n",
    "\n",
    "        if trace:\n",
    "            print(f\"[Episode {ep}] Action: {action}, Reward: {reward:.3f}\")\n",
    "\n",
    "        # Optional: decay exploration\n",
    "        agent.decay_epsilon()\n",
    "\n",
    "    return rewards_per_episode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373eab85",
   "metadata": {},
   "source": [
    "### Unit tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a28b0112",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Helper functions\n",
    "\n",
    "def generate_random_config() -> dict:\n",
    "    config = {\n",
    "        \"obd_parameters\": [],\n",
    "        \"global_settings\": {}\n",
    "    }\n",
    "\n",
    "    for pid in ACTIVE_PIDS:\n",
    "        enabled = random.choice([True, False])\n",
    "        strategy = random.choice(STRATEGY_LIST)\n",
    "        config[\"obd_parameters\"].append({\n",
    "            \"pid\": pid,\n",
    "            \"enabled\": enabled,\n",
    "            \"strategy\": strategy\n",
    "        })\n",
    "\n",
    "    config[\"global_settings\"] = {\n",
    "        \"min_time\": random.choice([5, 10, 30, 60, 120, 300]),\n",
    "        \"send_period\": random.choice([10, 30, 60, 120, 300]),\n",
    "        \"min_saved_records\": random.randint(1, 10)\n",
    "    }\n",
    "\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "160362d7",
   "metadata": {},
   "outputs": [
    {
     "ename": "StopIteration",
     "evalue": "Manual testing block below. Execution stopped.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mManual testing block below. Execution stopped.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mStopIteration\u001b[0m: Manual testing block below. Execution stopped."
     ]
    }
   ],
   "source": [
    "raise StopIteration(\"Manual testing block below. Execution stopped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d3d8d8",
   "metadata": {},
   "source": [
    "#### Encode / decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ff4d0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"obd_parameters\": [\n",
    "        {\"pid\": 12, \"enabled\": True, \"strategy\": \"on_change\"},\n",
    "        {\"pid\": 13, \"enabled\": True, \"strategy\": \"monitoring\"},\n",
    "        {\"pid\": 39, \"enabled\": False, \"strategy\": \"hysteresis\"},\n",
    "    ],\n",
    "    \"global_settings\": {\n",
    "        \"min_time\": 10,\n",
    "        \"send_period\": 60,\n",
    "        \"min_saved_records\": 1\n",
    "    }\n",
    "}\n",
    "\n",
    "vec = encode_config(config)\n",
    "decoded = decode_config(vec)\n",
    "assert config == decoded  # Should pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c47793",
   "metadata": {},
   "source": [
    "#### Apply action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "04989f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_apply_action_on_all_ids():\n",
    "    base_config = generate_random_config()\n",
    "    baseline_vector = encode_config(base_config)\n",
    "    print(\"Base Config:\", decode_config(baseline_vector))\n",
    "\n",
    "    n_pids = len(ACTIVE_PIDS)\n",
    "    total_actions = 2 * n_pids + 7\n",
    "\n",
    "    for action_id in range(total_actions):\n",
    "        print(f\"\\n--- Testing action {action_id} ---\")\n",
    "        new_vector, trace = apply_action(baseline_vector, action_id, trace=True, baseline=baseline_vector)\n",
    "\n",
    "        # Assert same length\n",
    "        assert len(new_vector) == len(baseline_vector), f\"Vector length changed for action {action_id}\"\n",
    "\n",
    "        # Assert valid strategy index\n",
    "        for i in range(n_pids):\n",
    "            strategy_idx = new_vector[i * 3 + 2]\n",
    "            assert 0 <= strategy_idx < len(STRATEGY_LIST), f\"Invalid strategy index {strategy_idx} after action {action_id}\"\n",
    "\n",
    "        # Global settings sanity check\n",
    "        g_base = n_pids * 3\n",
    "        assert new_vector[g_base] >= 1, \"min_time below 1\"\n",
    "        assert new_vector[g_base + 1] >= 1, \"send_period below 1\"\n",
    "        assert 1 <= new_vector[g_base + 2] <= 10, \"min_saved_records out of bounds\"\n",
    "\n",
    "        print(\"Trace:\", trace)\n",
    "        print(\"✅ Passed all assertions for action\", action_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ca352ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Config: {'obd_parameters': [{'pid': 12, 'enabled': False, 'strategy': 'monitoring'}, {'pid': 13, 'enabled': False, 'strategy': 'hysteresis'}, {'pid': 39, 'enabled': True, 'strategy': 'hysteresis'}], 'global_settings': {'min_time': 10, 'send_period': 10, 'min_saved_records': 8}}\n",
      "\n",
      "--- Testing action 0 ---\n",
      "[apply_action] Toggled PID 12 enable → 1\n",
      "Trace: {'change': 'Toggled PID 12 enable → 1'}\n",
      "✅ Passed all assertions for action 0\n",
      "\n",
      "--- Testing action 1 ---\n",
      "[apply_action] Toggled PID 13 enable → 1\n",
      "Trace: {'change': 'Toggled PID 13 enable → 1'}\n",
      "✅ Passed all assertions for action 1\n",
      "\n",
      "--- Testing action 2 ---\n",
      "[apply_action] Toggled PID 39 enable → 0\n",
      "Trace: {'change': 'Toggled PID 39 enable → 0'}\n",
      "✅ Passed all assertions for action 2\n",
      "\n",
      "--- Testing action 3 ---\n",
      "[apply_action] PID 12 strategy: monitoring → on_change\n",
      "Trace: {'change': 'PID 12 strategy: monitoring → on_change'}\n",
      "✅ Passed all assertions for action 3\n",
      "\n",
      "--- Testing action 4 ---\n",
      "[apply_action] PID 13 strategy: hysteresis → on_delta_change\n",
      "Trace: {'change': 'PID 13 strategy: hysteresis → on_delta_change'}\n",
      "✅ Passed all assertions for action 4\n",
      "\n",
      "--- Testing action 5 ---\n",
      "[apply_action] PID 39 strategy: hysteresis → on_delta_change\n",
      "Trace: {'change': 'PID 39 strategy: hysteresis → on_delta_change'}\n",
      "✅ Passed all assertions for action 5\n",
      "\n",
      "--- Testing action 6 ---\n",
      "[apply_action] min_time changed from 10 → 15\n",
      "Trace: {'change': 'min_time changed from 10 → 15'}\n",
      "✅ Passed all assertions for action 6\n",
      "\n",
      "--- Testing action 7 ---\n",
      "[apply_action] min_time changed from 10 → 5\n",
      "Trace: {'change': 'min_time changed from 10 → 5'}\n",
      "✅ Passed all assertions for action 7\n",
      "\n",
      "--- Testing action 8 ---\n",
      "[apply_action] send_period changed from 10 → 15\n",
      "Trace: {'change': 'send_period changed from 10 → 15'}\n",
      "✅ Passed all assertions for action 8\n",
      "\n",
      "--- Testing action 9 ---\n",
      "[apply_action] send_period changed from 10 → 5\n",
      "Trace: {'change': 'send_period changed from 10 → 5'}\n",
      "✅ Passed all assertions for action 9\n",
      "\n",
      "--- Testing action 10 ---\n",
      "[apply_action] min_saved_records changed from 8 → 9\n",
      "Trace: {'change': 'min_saved_records changed from 8 → 9'}\n",
      "✅ Passed all assertions for action 10\n",
      "\n",
      "--- Testing action 11 ---\n",
      "[apply_action] min_saved_records changed from 8 → 7\n",
      "Trace: {'change': 'min_saved_records changed from 8 → 7'}\n",
      "✅ Passed all assertions for action 11\n",
      "\n",
      "--- Testing action 12 ---\n",
      "[apply_action] Reset to baseline configuration\n",
      "Trace: {'change': 'Reset to baseline configuration'}\n",
      "✅ Passed all assertions for action 12\n"
     ]
    }
   ],
   "source": [
    "test_apply_action_on_all_ids()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3daff572",
   "metadata": {},
   "source": [
    "#### Q-Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5462d64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Baseline configuration run:  \"2025-05-16T06:40:38Z\", \"2025-05-17T23:59:00Z\"  \n",
    "# There are other 2 runs with baseline config on the last days of april 27th to 29th \n",
    "# TODO get exact baseline configuration parameters.\n",
    "\n",
    "### Worst configuration run:  \"2025-05-16T06:40:38Z\", \"2025-05-17T23:59:00Z\"\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4aedb1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_qagent_basic():\n",
    "    agent = QAgent(action_space_size=13, epsilon=0.0)  # deterministic\n",
    "    state = encode_config({\n",
    "        \"obd_parameters\": [\n",
    "            {\"pid\": 12, \"enabled\": True, \"strategy\": \"on_change\"},\n",
    "            {\"pid\": 13, \"enabled\": True, \"strategy\": \"on_change\"},\n",
    "            {\"pid\": 39, \"enabled\": True, \"strategy\": \"on_change\"}\n",
    "        ],\n",
    "        \"global_settings\": {\"min_time\": 10, \"send_period\": 60, \"min_saved_records\": 1}\n",
    "    })\n",
    "\n",
    "    action = agent.select_action(state)\n",
    "    next_state, _ = apply_action(state, action, baseline=state)\n",
    "    reward = 2.5\n",
    "    agent.update(state, action, reward, next_state)\n",
    "\n",
    "    # Assert Q-value updated\n",
    "    state_key = tuple(state)\n",
    "    assert action < len(agent.q_table[state_key]), \"Q-table entry missing\"\n",
    "    q_value = agent.q_table[state_key][action]\n",
    "    assert q_value != 0.0, \"Q-value not updated\"\n",
    "    print(f\"✅ Q-table updated: Q[state][{action}] = {q_value:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "70372d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[apply_action] Toggled PID 12 enable → 0\n",
      "✅ Q-table updated: Q[state][0] = 1.250\n"
     ]
    }
   ],
   "source": [
    "test_qagent_basic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "83df0d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_qagent_training():\n",
    "    baseline_config = {\n",
    "        \"obd_parameters\": [\n",
    "            {\"pid\": 12, \"enabled\": True, \"strategy\": \"monitoring\"},\n",
    "            {\"pid\": 13, \"enabled\": True, \"strategy\": \"monitoring\"},\n",
    "            {\"pid\": 39, \"enabled\": True, \"strategy\": \"monitoring\"}\n",
    "        ],\n",
    "        \"global_settings\": {\"min_time\": 300, \"send_period\": 120, \"min_saved_records\": 1}\n",
    "    }\n",
    "\n",
    "    baseline_vector = encode_config(baseline_config)\n",
    "    agent = QAgent(action_space_size=13, epsilon=0.3)\n",
    "\n",
    "    rewards = train_q_agent(\n",
    "        agent,\n",
    "        baseline_vector,\n",
    "        start_ts=\"2025-05-16T06:40:38Z\",\n",
    "        end_ts=\"2025-05-17T23:59:00Z\"  ,\n",
    "        freq_csv_path=\"../../data_proc/csv_data/qa_device/frequencies.csv\",\n",
    "        obd_csv_path=\"../../data_proc/csv_data/qa_device/obd_export.csv\",\n",
    "        episodes=5,\n",
    "        trace=True\n",
    "    )\n",
    "\n",
    "    assert len(rewards) == 5, \"Incorrect number of training episodes\"\n",
    "    assert all(isinstance(r, (float, int)) for r in rewards), \"Non-numeric reward detected\"\n",
    "    print(\"✅ Training rewards:\", rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a4ffeb36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Episode 0] Action: 0, Reward: 1.700\n",
      "[Episode 1] Action: 0, Reward: 1.700\n",
      "[Episode 2] Action: 0, Reward: 1.700\n",
      "[Episode 3] Action: 0, Reward: 1.700\n",
      "[Episode 4] Action: 0, Reward: 1.700\n",
      "✅ Training rewards: [1.7, 1.7, 1.7, 1.7, 1.7]\n"
     ]
    }
   ],
   "source": [
    "test_qagent_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c28be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training\n",
    "\n",
    "baseline_config = {\n",
    "    \"obd_parameters\": [\n",
    "        {\"pid\": 12, \"enabled\": True, \"strategy\": \"monitoring\"},\n",
    "        {\"pid\": 13, \"enabled\": True, \"strategy\": \"monitoring\"},\n",
    "        {\"pid\": 39, \"enabled\": True, \"strategy\": \"monitoring\"}\n",
    "    ],\n",
    "    \"global_settings\": {\"min_time\": 300, \"send_period\": 120, \"min_saved_records\": 1}\n",
    "}\n",
    "\n",
    "baseline_vector = encode_config(baseline_config)\n",
    "\n",
    "agent = QAgent(action_space_size=13, epsilon=0.3)\n",
    "rewards = train_q_agent(\n",
    "    agent,\n",
    "    baseline_vector,\n",
    "    start_ts=\"2025-05-23T13:55:00Z\",\n",
    "    end_ts=\"2025-05-23T14:07:00Z\",\n",
    "    freq_csv_path=\"frequencies.csv\",\n",
    "    obd_csv_path=\"obd_export.csv\"\n",
    ")\n",
    "\n",
    "plot_rewards(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883353b0",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27132366",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Implementation references**\n",
    "\n",
    "- OpenAI Spinning Up: https://spinningup.openai.com\n",
    "Although it focuses more on policy-gradient methods, it gives good context on where Q-learning fits in the broader RL ecosystem.\n",
    "\n",
    "- RL Course by David Silver (DeepMind)\n",
    "Lectures 4–6 cover model-free methods, including Q-Learning.\n",
    "\n",
    "- Towards Data Science\n",
    "https://towardsdatascience.com/reinforcement-learning-explained-visually-part-4-q-learning-step-by-step-b65efb731d3e/\n",
    "\n",
    "\n",
    "\n",
    "**Academic references**\n",
    "\n",
    "1. Watkins, C.J.C.H., & Dayan, P. (1992)\n",
    "   Q-learning: https://link.springer.com/article/10.1007/BF00992698\n",
    "\n",
    "   \n",
    "2. Sutton, R. S., & Barto, A. G. (2018)\n",
    "    Reinforcement Learning: An Introduction (2nd Edition)\n",
    "    Chapter 6 covers Q-Learning in depth.\n",
    "    http://incompleteideas.net/book/the-book-2nd.html "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aau",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
